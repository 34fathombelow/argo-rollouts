{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Argo Rollouts - Advanced Kubernetes Deployment Controller What is Argo Rollouts? Argo Rollouts introduces a new custom resource called a Rollout to provide additional deployment strategies such as Blue Green and Canary to Kubernetes. The Rollout custom resource provides feature parity with the deployment resource with additional deployment strategies. Check out the Deployment Concepts for more information on the various deployment strategies. Why Argo Rollouts? Deployments resources offer two strategies to deploy changes: RollingUpdate and Recreate . While these strategies can solve a wide number of use cases, large scale production deployments use additional strategies, such as blue-green or canary, that are missing from the Deployment controller. In order to use these strategies in Kubernetes, users are forced to build scripts on top of their deployments. The Argo Rollouts controller provides these strategies as simple declarative, configurable, GitOps-friendly options. Getting Started Quick Start Argo Rollouts can be installed by running the following commands: $ kubectl create namespace argo-rollouts $ kubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml Check out the getting started guide to walk through creating and then updating a rollout object. How does it work? Similar to the deployment object, the Argo Rollouts controller will manage the creation, scaling, and deletion of ReplicaSets. These ReplicaSets are defined by the spec.template field, which uses the same pod template as the deployment object. When the spec.template is changed, that signals to the Argo Rollouts controller that a new ReplicaSet will be introduced. The controller will use the strategy set within the spec.strategy field in order to determine how the rollout will progress from the old ReplicaSet to the new ReplicaSet. Once that new ReplicaSet has successfully progressed into the stable version, that Rollout will be marked as the stable ReplicaSet. If another change occurs in the spec.template during a transition from a stable ReplicaSet to a new ReplicaSet. The previously new ReplicaSet will be scaled down, and the controller will try to progress the ReplicasSet that reflects the spec.template field. There is more information on the behaviors of each strategy in the spec section. Use cases of Argo Rollouts A user wants to run last minute functional tests on the new version before it starts to serve production traffic. With the BlueGreen strategy, Argo Rollouts allow users to specify a preview service and an active service. The Rollout will configure the preview service to send traffic to the new version while the active service continues to receive production traffic. Once a user is satisfied, they can promote the preview service to be the new active service. ( example ) Before a new version starts receiving live traffic, a generic set of steps need to be executed beforehand. With the BlueGreen Strategy, the user can bring up the new version without it receiving traffic from the active service. Once those steps finish executing, the rollout can cut over traffic to the new version. A user wants to give a small percentage of the production traffic to a new version of their application for a couple of hours. Afterward, they want to scale down the new version and look at some metrics to determine if the new version is performant compared to the old version. Then they will decide if they want to rollout the new version for all of the production traffic or stick with the current version. With the canary strategy, the rollout can scale up a replica with the new version to receive a specified percentage of traffic, wait for a specified amount of time, set the percentage back to 0, and then wait to rollout out to service all of the traffic once the user is satisfied. ( example ) A user wants to slowly give the new version more production traffic. They start by giving it a small percentage of the live traffic and wait a while before giving the new version more traffic. Eventually, the new version will receive all the production traffic. With the canary strategy, the user specifies the percentages they want the new version to receive and the amount of time to wait between percentages. ( example A user wants to use the normal Rolling Update strategy from the deployment. If a user uses the canary strategy with no steps, the rollout will use the max surge and max unavailable values to roll to the new version. ( example )","title":"Overview"},{"location":"#argo-rollouts-advanced-kubernetes-deployment-controller","text":"","title":"Argo Rollouts - Advanced Kubernetes Deployment Controller"},{"location":"#what-is-argo-rollouts","text":"Argo Rollouts introduces a new custom resource called a Rollout to provide additional deployment strategies such as Blue Green and Canary to Kubernetes. The Rollout custom resource provides feature parity with the deployment resource with additional deployment strategies. Check out the Deployment Concepts for more information on the various deployment strategies.","title":"What is Argo Rollouts?"},{"location":"#why-argo-rollouts","text":"Deployments resources offer two strategies to deploy changes: RollingUpdate and Recreate . While these strategies can solve a wide number of use cases, large scale production deployments use additional strategies, such as blue-green or canary, that are missing from the Deployment controller. In order to use these strategies in Kubernetes, users are forced to build scripts on top of their deployments. The Argo Rollouts controller provides these strategies as simple declarative, configurable, GitOps-friendly options.","title":"Why Argo Rollouts?"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#quick-start","text":"Argo Rollouts can be installed by running the following commands: $ kubectl create namespace argo-rollouts $ kubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml Check out the getting started guide to walk through creating and then updating a rollout object.","title":"Quick Start"},{"location":"#how-does-it-work","text":"Similar to the deployment object, the Argo Rollouts controller will manage the creation, scaling, and deletion of ReplicaSets. These ReplicaSets are defined by the spec.template field, which uses the same pod template as the deployment object. When the spec.template is changed, that signals to the Argo Rollouts controller that a new ReplicaSet will be introduced. The controller will use the strategy set within the spec.strategy field in order to determine how the rollout will progress from the old ReplicaSet to the new ReplicaSet. Once that new ReplicaSet has successfully progressed into the stable version, that Rollout will be marked as the stable ReplicaSet. If another change occurs in the spec.template during a transition from a stable ReplicaSet to a new ReplicaSet. The previously new ReplicaSet will be scaled down, and the controller will try to progress the ReplicasSet that reflects the spec.template field. There is more information on the behaviors of each strategy in the spec section.","title":"How does it work?"},{"location":"#use-cases-of-argo-rollouts","text":"A user wants to run last minute functional tests on the new version before it starts to serve production traffic. With the BlueGreen strategy, Argo Rollouts allow users to specify a preview service and an active service. The Rollout will configure the preview service to send traffic to the new version while the active service continues to receive production traffic. Once a user is satisfied, they can promote the preview service to be the new active service. ( example ) Before a new version starts receiving live traffic, a generic set of steps need to be executed beforehand. With the BlueGreen Strategy, the user can bring up the new version without it receiving traffic from the active service. Once those steps finish executing, the rollout can cut over traffic to the new version. A user wants to give a small percentage of the production traffic to a new version of their application for a couple of hours. Afterward, they want to scale down the new version and look at some metrics to determine if the new version is performant compared to the old version. Then they will decide if they want to rollout the new version for all of the production traffic or stick with the current version. With the canary strategy, the rollout can scale up a replica with the new version to receive a specified percentage of traffic, wait for a specified amount of time, set the percentage back to 0, and then wait to rollout out to service all of the traffic once the user is satisfied. ( example ) A user wants to slowly give the new version more production traffic. They start by giving it a small percentage of the live traffic and wait a while before giving the new version more traffic. Eventually, the new version will receive all the production traffic. With the canary strategy, the user specifies the percentages they want the new version to receive and the amount of time to wait between percentages. ( example A user wants to use the normal Rolling Update strategy from the deployment. If a user uses the canary strategy with no steps, the rollout will use the max surge and max unavailable values to roll to the new version. ( example )","title":"Use cases of Argo Rollouts"},{"location":"CONTRIBUTING/","text":"Contributing Before You Start Argo Rollouts is written in Golang. If you do not have a good grounding in Go, try out the tutorial . Pre-requisites Install: docker golang dep kubectl kustomize minikube or Docker for Desktop Argo Rollout additionally uses * controller-gen binary in order to auto-generate the crd manifest * golangci-lint to lint the project. Run the following commands to install them: go get -u github.com/kubernetes-sigs/controller-tools/cmd/controller-gen go get -u github.com/golangci/golangci-lint/cmd/golangci-lint Brew users can quickly install the lot: brew install go dep kubectl kustomize Set up environment variables (e.g. is ~/.bashrc ): export GOPATH = ~/go export PATH = $PATH : $GOPATH /bin Checkout the code: go get -u github.com/argoproj/argo-rollouts cd ~/go/src/github.com/argoproj/argo-rollouts Building Ensure dependencies are up to date first: dep ensure -v The make controller command will build the controller. make codegen - Runs the code generator that creates the informers, client, lister, and deepcopies from the types.go and modifies the open-api spec. Running Tests To run unit tests: make test Running Locally It is much easier to run and debug if you run Argo Rollout in your local machine than in the Kubernetes cluster. cd ~/go/src/github.com/argoproj/argo-rollouts make controller ./dist/rollouts-controller Running Local Containers You may need to run containers locally, so here's how: Create login to Docker Hub, then login. docker login Add your username as the environment variable, e.g. to your ~/.bash_profile : export IMAGE_NAMESPACE = argoproj Build the images: DOCKER_PUSH = true make image Update the manifests: make manifests Install the manifests: kubectl -n argo-rollouts apply -f manifests/install.yaml Documentation Changes If you need to run the mkdocs server, you will need to do the following: Follow the instruction guide to install mkDocs Install the material theme with the following guide Afterwards, you can run mkdocs serve and access your documentation at http://127.0.0.1:8000/ If you don't want to setup mkDocs locally, the following docker command should suffice: docker run --rm -it -p 8000 :8000 -v ${ PWD } :/docs squidfunk/mkdocs-material","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"","title":"Contributing"},{"location":"CONTRIBUTING/#before-you-start","text":"Argo Rollouts is written in Golang. If you do not have a good grounding in Go, try out the tutorial .","title":"Before You Start"},{"location":"CONTRIBUTING/#pre-requisites","text":"Install: docker golang dep kubectl kustomize minikube or Docker for Desktop Argo Rollout additionally uses * controller-gen binary in order to auto-generate the crd manifest * golangci-lint to lint the project. Run the following commands to install them: go get -u github.com/kubernetes-sigs/controller-tools/cmd/controller-gen go get -u github.com/golangci/golangci-lint/cmd/golangci-lint Brew users can quickly install the lot: brew install go dep kubectl kustomize Set up environment variables (e.g. is ~/.bashrc ): export GOPATH = ~/go export PATH = $PATH : $GOPATH /bin Checkout the code: go get -u github.com/argoproj/argo-rollouts cd ~/go/src/github.com/argoproj/argo-rollouts","title":"Pre-requisites"},{"location":"CONTRIBUTING/#building","text":"Ensure dependencies are up to date first: dep ensure -v The make controller command will build the controller. make codegen - Runs the code generator that creates the informers, client, lister, and deepcopies from the types.go and modifies the open-api spec.","title":"Building"},{"location":"CONTRIBUTING/#running-tests","text":"To run unit tests: make test","title":"Running Tests"},{"location":"CONTRIBUTING/#running-locally","text":"It is much easier to run and debug if you run Argo Rollout in your local machine than in the Kubernetes cluster. cd ~/go/src/github.com/argoproj/argo-rollouts make controller ./dist/rollouts-controller","title":"Running Locally"},{"location":"CONTRIBUTING/#running-local-containers","text":"You may need to run containers locally, so here's how: Create login to Docker Hub, then login. docker login Add your username as the environment variable, e.g. to your ~/.bash_profile : export IMAGE_NAMESPACE = argoproj Build the images: DOCKER_PUSH = true make image Update the manifests: make manifests Install the manifests: kubectl -n argo-rollouts apply -f manifests/install.yaml","title":"Running Local Containers"},{"location":"CONTRIBUTING/#documentation-changes","text":"If you need to run the mkdocs server, you will need to do the following: Follow the instruction guide to install mkDocs Install the material theme with the following guide Afterwards, you can run mkdocs serve and access your documentation at http://127.0.0.1:8000/ If you don't want to setup mkDocs locally, the following docker command should suffice: docker run --rm -it -p 8000 :8000 -v ${ PWD } :/docs squidfunk/mkdocs-material","title":"Documentation Changes"},{"location":"deployment-concepts/","text":"Deployment Concepts While the industry has agreed upon high-level definitions of various deployment strategies, the implementations of these strategies tend to differ across tooling. To make it clear how the Argo Rollouts will behave, here are the descriptions of the various deployment strategies implementations offered by the Argo Rollouts. Rolling Update A RollingUpdate slowly replaces the old version with the new version. As the new version comes up, the old version is scaled down in order to maintain the overall count of the application. This is the default strategy of the deployment object. Recreate A Recreate deployment deletes the old version of the application before bring up the new version. As a result, this ensures that two versions of the application never run at the same time, but there is downtime during the deployment. Blue Green A Blue Green deployment (which is sometimes referred to as a Red-Black) has both the new and old version of the application deployed at the same time. During this time, only the old version of the application will receive production traffic. This allows the developers to run tests against the new version before switching the live traffic to the new version. Canary A Canary deployment exposes a subset of users (usually 5%) to the new version of the application while serving the rest of the traffic to the old version. Once the new version is verified to be correct that the new version can gradually replace the old version. Service Meshes like Istio make canary deployments easily to rollout as the service mesh can filter the traffic for you.","title":"Concepts"},{"location":"deployment-concepts/#deployment-concepts","text":"While the industry has agreed upon high-level definitions of various deployment strategies, the implementations of these strategies tend to differ across tooling. To make it clear how the Argo Rollouts will behave, here are the descriptions of the various deployment strategies implementations offered by the Argo Rollouts.","title":"Deployment Concepts"},{"location":"deployment-concepts/#rolling-update","text":"A RollingUpdate slowly replaces the old version with the new version. As the new version comes up, the old version is scaled down in order to maintain the overall count of the application. This is the default strategy of the deployment object.","title":"Rolling Update"},{"location":"deployment-concepts/#recreate","text":"A Recreate deployment deletes the old version of the application before bring up the new version. As a result, this ensures that two versions of the application never run at the same time, but there is downtime during the deployment.","title":"Recreate"},{"location":"deployment-concepts/#blue-green","text":"A Blue Green deployment (which is sometimes referred to as a Red-Black) has both the new and old version of the application deployed at the same time. During this time, only the old version of the application will receive production traffic. This allows the developers to run tests against the new version before switching the live traffic to the new version.","title":"Blue Green"},{"location":"deployment-concepts/#canary","text":"A Canary deployment exposes a subset of users (usually 5%) to the new version of the application while serving the rest of the traffic to the old version. Once the new version is verified to be correct that the new version can gradually replace the old version. Service Meshes like Istio make canary deployments easily to rollout as the service mesh can filter the traffic for you.","title":"Canary"},{"location":"getting-started/","text":"Getting Started Requirements Installed kubectl command-line tool Have a kubeconfig file (default location is ~/.kube/config). Install Argo Rollouts Argo Rollouts can be installed at a cluster or namespace level. Cluster-Level installation kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml This will create a new namespace, argo-rollouts , where Argo Rollouts controller will live. On GKE, you will need grant your account the ability to create new cluster roles: kubectl create clusterrolebinding YOURNAME-cluster-admin-binding --clusterrole = cluster-admin --user = YOUREMAIL@gmail.com Note: The cluster-level installation assumes that Argo Rollouts is deployed into the argo-rollouts namespace. If you would like to install Argo Rollouts in another namespace, you will need to modify the ClusterRoleBinding resource that binds the ClusterRole to the ServiceAcccount created. The namespace for the ServiceAccount referenced in the ClusterRoleBinding needs to be modified to match your desired namespace. Namespace-Level Installation kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/namespace-install.yaml Converting Deployment to Rollout Converting a Deployment to a Rollout simply is a core design principle of Argo Rollouts. There are two key changes: Changing the apiVersion value to argoproj.io/v1alpha1 and changing the kind value from Deployment to Rollout Adding a new deployment strategy to the new Rollout. You can read up on the available strategies at Argo Rollouts section Below is an example of a Rollout YAML using the Canary strategy. apiVersion : argoproj.io/v1alpha1 # Changed from apps/v1 kind : Rollout # Changed from Deployment # ----- Everything below this comment is the same as a deployment ----- metadata : name : example-rollout spec : replicas : 5 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.15.4 ports : - containerPort : 80 minReadySeconds : 30 revisionHistoryLimit : 3 strategy : # ----- Everything above this comment are the same as a deployment ----- canary : # A new field that used to provide configurable options for a Canary strategy steps : - setWeight : 20 - pause : {} Updating the Rollout The initial creation of the above Rollout will bring up all 5 replicas of the Pod Spec listed. Since the rollout was not in a stable state beforehand (as it was just created), the rollout will skip the steps listed in the .spec.strategy.canary.steps field to first become stable. Once the new ReplicaSet is healthy, updating any field in the spec.template will cause the rollout to create a new ReplicaSet and execute the steps in spec.strategy.canary.steps to transition to the new version. To demonstrate this, we will update the rollout to use a new nginx image. You can either run kubectl edit rollout example-rollout and change the image from nginx:1.15.4 to nginx:1.15.5 , or run the following: $ kubectl patch rollout example-rollout --type merge -p { spec : { template : { spec : { containers : [{ name : nginx , image : nginx:1.15.5 }]}}}} Once the patch is applied, you can watch the new replicaset came up as healthy by running $ kubectl get replicaset -w -o wide Once that replicaset is healthy, the rollout will enter a paused state by adding a pause condition to .status.pauseConditions . The pause condition contains a reason and a pause start time. Promoting the rollout The rollout does not continue progessing to the new version until the pause conditon is removed from the status. Since the rollout YAML submitted does not have a duration within the pause step, the Rollout is paused indefinitely until a external process (i.e. a user or automiated tool) removes the pause conditon. Argo Rollouts has a kubectl plugin to help automate operations like promoting a rollout through a step. The installation instructions are here . Once the plugin is installed, the user can run the following command to promote the rollout through the pause step: kubectl argo rollouts promote example-rollout At this point, the Rollout has executed all the steps to transition to a new version. As a result, the new ReplicaSet is considered the new stable ReplicaSet, and the previous ReplicaSet will be scaled down. The Rollout will repeat these steps when the Pod Spec Template is changed again. Going forward Check out the features page for more configuration options for a rollout.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#requirements","text":"Installed kubectl command-line tool Have a kubeconfig file (default location is ~/.kube/config).","title":"Requirements"},{"location":"getting-started/#install-argo-rollouts","text":"Argo Rollouts can be installed at a cluster or namespace level.","title":"Install Argo Rollouts"},{"location":"getting-started/#cluster-level-installation","text":"kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml This will create a new namespace, argo-rollouts , where Argo Rollouts controller will live. On GKE, you will need grant your account the ability to create new cluster roles: kubectl create clusterrolebinding YOURNAME-cluster-admin-binding --clusterrole = cluster-admin --user = YOUREMAIL@gmail.com Note: The cluster-level installation assumes that Argo Rollouts is deployed into the argo-rollouts namespace. If you would like to install Argo Rollouts in another namespace, you will need to modify the ClusterRoleBinding resource that binds the ClusterRole to the ServiceAcccount created. The namespace for the ServiceAccount referenced in the ClusterRoleBinding needs to be modified to match your desired namespace.","title":"Cluster-Level installation"},{"location":"getting-started/#namespace-level-installation","text":"kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/namespace-install.yaml","title":"Namespace-Level Installation"},{"location":"getting-started/#converting-deployment-to-rollout","text":"Converting a Deployment to a Rollout simply is a core design principle of Argo Rollouts. There are two key changes: Changing the apiVersion value to argoproj.io/v1alpha1 and changing the kind value from Deployment to Rollout Adding a new deployment strategy to the new Rollout. You can read up on the available strategies at Argo Rollouts section Below is an example of a Rollout YAML using the Canary strategy. apiVersion : argoproj.io/v1alpha1 # Changed from apps/v1 kind : Rollout # Changed from Deployment # ----- Everything below this comment is the same as a deployment ----- metadata : name : example-rollout spec : replicas : 5 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.15.4 ports : - containerPort : 80 minReadySeconds : 30 revisionHistoryLimit : 3 strategy : # ----- Everything above this comment are the same as a deployment ----- canary : # A new field that used to provide configurable options for a Canary strategy steps : - setWeight : 20 - pause : {}","title":"Converting Deployment to Rollout"},{"location":"getting-started/#updating-the-rollout","text":"The initial creation of the above Rollout will bring up all 5 replicas of the Pod Spec listed. Since the rollout was not in a stable state beforehand (as it was just created), the rollout will skip the steps listed in the .spec.strategy.canary.steps field to first become stable. Once the new ReplicaSet is healthy, updating any field in the spec.template will cause the rollout to create a new ReplicaSet and execute the steps in spec.strategy.canary.steps to transition to the new version. To demonstrate this, we will update the rollout to use a new nginx image. You can either run kubectl edit rollout example-rollout and change the image from nginx:1.15.4 to nginx:1.15.5 , or run the following: $ kubectl patch rollout example-rollout --type merge -p { spec : { template : { spec : { containers : [{ name : nginx , image : nginx:1.15.5 }]}}}} Once the patch is applied, you can watch the new replicaset came up as healthy by running $ kubectl get replicaset -w -o wide Once that replicaset is healthy, the rollout will enter a paused state by adding a pause condition to .status.pauseConditions . The pause condition contains a reason and a pause start time.","title":"Updating the Rollout"},{"location":"getting-started/#promoting-the-rollout","text":"The rollout does not continue progessing to the new version until the pause conditon is removed from the status. Since the rollout YAML submitted does not have a duration within the pause step, the Rollout is paused indefinitely until a external process (i.e. a user or automiated tool) removes the pause conditon. Argo Rollouts has a kubectl plugin to help automate operations like promoting a rollout through a step. The installation instructions are here . Once the plugin is installed, the user can run the following command to promote the rollout through the pause step: kubectl argo rollouts promote example-rollout At this point, the Rollout has executed all the steps to transition to a new version. As a result, the new ReplicaSet is considered the new stable ReplicaSet, and the previous ReplicaSet will be scaled down. The Rollout will repeat these steps when the Pod Spec Template is changed again.","title":"Promoting the rollout"},{"location":"getting-started/#going-forward","text":"Check out the features page for more configuration options for a rollout.","title":"Going forward"},{"location":"features/","text":"Overview The Rollout object has two available strategies: Canary and BlueGreen. Below are the links to the documenation for each strategy: Blue Green Canary The following describes all the available fields of a rollout: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : example-rollout-canary spec : # Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1. replicas : 5 # Label selector for pods. Existing ReplicaSets whose pods are selected by this will be the ones affected by this rollout. It must match the pod template s labels.` selector : matchLabels : app : guestbook # Template describes the pods that will be created. Same as deployment template : spec : containers : - name : guestbook image : gcr.io/heptio-images/ks-guestbook-demo:0.1 # Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available. Defaults to 0 (pod will be considered available as soon as it is ready) minReadySeconds : 30 # The number of old ReplicaSets to retain. If unspecified, will retain 10 old ReplicaSets revisionHistoryLimit : 3 # Indiciates if the rollout is paused paused : false # The maximum time in seconds for a rollout to make progress before it is considered to be failed. Argo Rollouts will continue to process failed rollouts and a condition with a ProgressDeadlineExceeded reason will be surfaced in the rollout status. Note that progress will not be estimated during the time a rollout is paused. Defaults to 600s. progressDeadlineSeconds : 600 # Field to specify the strategy to run strategy : blueGreen : # Name of the service that the rollout modifies as the active service. activeService : active-service # Name of the service that the rollout modifies as the preview service. previewService : preview-service # The number of replicas to run under the preview service before the switchover. Once the rollout is resumed the new replicaset will be full scaled up before the switch occurs +optional previewReplicaCount : 1 # Indicates if the rollout should automatically promote the new ReplicaSet to the active service or enter a paused state. If not specified, the default value is true. +optional autoPromotionEnabled : false # Automatically promotes the current ReplicaSet to active after the specified pause delay in seconds after the ReplicaSet becomes ready. If omitted, the Rollout enters and remains in a paused state until manually resumed by resetting spec.Paused to false. +optional autoPromotionSeconds : 30 # Adds a delay before scaling down the previous replicaset. If omitted, the Rollout waits 30 seconds before scaling down the previous ReplicaSet. A minimum of 30 seconds is recommended to ensure IP table propagation across the nodes in a cluster. See https://github.com/argoproj/argo-rollouts/issues/19#issuecomment-476329960 for more information scaleDownDelaySeconds : 30 # Limits the number of old RS that can run at once before getting scaled down. Defaults to nil scaleDownDelayRevisionLimit : 2 canary : # CanaryService holds the name of a service which selects pods with canary version and don t select any pods with stable version. +optional canaryService : canary-service # The maximum number of pods that can be unavailable during the update. Value can be an absolute number (ex: 5) or a percentage of total pods at the start of update (ex: 10%). Absolute number is calculated from percentage by rounding down. This can not be 0 if MaxSurge is 0. By default, a fixed value of 1 is used. Example: when this is set to 30%, the old RC can be scaled down by 30% immediately when the rolling update starts. Once new pods are ready, old RC can be scaled down further, followed by scaling up the new RC, ensuring that at least 70% of original number of pods are available at all times during the update. +optional maxUnavailable : 1 # The maximum number of pods that can be scheduled above the original number of pods. Value can be an absolute number (ex: 5) or a percentage of total pods at the start of the update (ex: 10%). This can not be 0 if MaxUnavailable is 0. Absolute number is calculated from percentage by rounding up. By default, a value of 1 is used. Example: when this is set to 30%, the new RC can be scaled up by 30% immediately when the rolling update starts. Once old pods have been killed, new RC can be scaled up further, ensuring that total number of pods running at any time during the update is atmost 130% of original pods. +optional maxSurge : 20% # Define the order of phases to execute the canary deployment +optional steps : # Sets the ratio of new replicasets to 20% - setWeight : 20 # Pauses the rollout for an hour - pause : duration : 3600 # One hour - setWeight : 40 # Sets .spec.paused to true and waits until the field is changed back - pause : {} status : pauseConditions : - reason : StepPause startTime : 2019-10-00T1234 - reason : BlueGreenPause startTime : 2019-10-00T1234 - reason : AnalysisRunInconclusive startTime : 2019-10-00T1234","title":"Overview"},{"location":"features/#overview","text":"The Rollout object has two available strategies: Canary and BlueGreen. Below are the links to the documenation for each strategy: Blue Green Canary The following describes all the available fields of a rollout: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : example-rollout-canary spec : # Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1. replicas : 5 # Label selector for pods. Existing ReplicaSets whose pods are selected by this will be the ones affected by this rollout. It must match the pod template s labels.` selector : matchLabels : app : guestbook # Template describes the pods that will be created. Same as deployment template : spec : containers : - name : guestbook image : gcr.io/heptio-images/ks-guestbook-demo:0.1 # Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available. Defaults to 0 (pod will be considered available as soon as it is ready) minReadySeconds : 30 # The number of old ReplicaSets to retain. If unspecified, will retain 10 old ReplicaSets revisionHistoryLimit : 3 # Indiciates if the rollout is paused paused : false # The maximum time in seconds for a rollout to make progress before it is considered to be failed. Argo Rollouts will continue to process failed rollouts and a condition with a ProgressDeadlineExceeded reason will be surfaced in the rollout status. Note that progress will not be estimated during the time a rollout is paused. Defaults to 600s. progressDeadlineSeconds : 600 # Field to specify the strategy to run strategy : blueGreen : # Name of the service that the rollout modifies as the active service. activeService : active-service # Name of the service that the rollout modifies as the preview service. previewService : preview-service # The number of replicas to run under the preview service before the switchover. Once the rollout is resumed the new replicaset will be full scaled up before the switch occurs +optional previewReplicaCount : 1 # Indicates if the rollout should automatically promote the new ReplicaSet to the active service or enter a paused state. If not specified, the default value is true. +optional autoPromotionEnabled : false # Automatically promotes the current ReplicaSet to active after the specified pause delay in seconds after the ReplicaSet becomes ready. If omitted, the Rollout enters and remains in a paused state until manually resumed by resetting spec.Paused to false. +optional autoPromotionSeconds : 30 # Adds a delay before scaling down the previous replicaset. If omitted, the Rollout waits 30 seconds before scaling down the previous ReplicaSet. A minimum of 30 seconds is recommended to ensure IP table propagation across the nodes in a cluster. See https://github.com/argoproj/argo-rollouts/issues/19#issuecomment-476329960 for more information scaleDownDelaySeconds : 30 # Limits the number of old RS that can run at once before getting scaled down. Defaults to nil scaleDownDelayRevisionLimit : 2 canary : # CanaryService holds the name of a service which selects pods with canary version and don t select any pods with stable version. +optional canaryService : canary-service # The maximum number of pods that can be unavailable during the update. Value can be an absolute number (ex: 5) or a percentage of total pods at the start of update (ex: 10%). Absolute number is calculated from percentage by rounding down. This can not be 0 if MaxSurge is 0. By default, a fixed value of 1 is used. Example: when this is set to 30%, the old RC can be scaled down by 30% immediately when the rolling update starts. Once new pods are ready, old RC can be scaled down further, followed by scaling up the new RC, ensuring that at least 70% of original number of pods are available at all times during the update. +optional maxUnavailable : 1 # The maximum number of pods that can be scheduled above the original number of pods. Value can be an absolute number (ex: 5) or a percentage of total pods at the start of the update (ex: 10%). This can not be 0 if MaxUnavailable is 0. Absolute number is calculated from percentage by rounding up. By default, a value of 1 is used. Example: when this is set to 30%, the new RC can be scaled up by 30% immediately when the rolling update starts. Once old pods have been killed, new RC can be scaled up further, ensuring that total number of pods running at any time during the update is atmost 130% of original pods. +optional maxSurge : 20% # Define the order of phases to execute the canary deployment +optional steps : # Sets the ratio of new replicasets to 20% - setWeight : 20 # Pauses the rollout for an hour - pause : duration : 3600 # One hour - setWeight : 40 # Sets .spec.paused to true and waits until the field is changed back - pause : {} status : pauseConditions : - reason : StepPause startTime : 2019-10-00T1234 - reason : BlueGreenPause startTime : 2019-10-00T1234 - reason : AnalysisRunInconclusive startTime : 2019-10-00T1234","title":"Overview"},{"location":"features/analysis/","text":"Canary Analysis Progressive Delivery Argo Rollouts provides several ways to perform canary analysis to drive progressive delivery. This document describes how to achieve various forms of progressive delivery, varying the point in time analysis is performed, it's frequency, and occurrence. Custom Resource Definitions CRD Description Rollout A Rollout acts as a drop-in replacement for a Deployment resource. It provides additional blueGreen and canary update strategies. These strategies can create AnalysisRuns and Experiments during the update, which will progress the update, or abort it. AnalysisTemplate An AnalysisTemplate is a template spec which defines how to perform a canary analysis, such as the metrics which it should perform, its frequency, and the values which are considered successful or failed. AnalysisTemplates may be parameterized with inputs values. AnalysisRun An AnalysisRun is an instantiation of an AnalysisTemplate . AnalysisRuns are like Jobs in that they eventually complete. Completed runs are considered Successful, Failed, or Inconclusive, and the result of the run affect if the Rollout's update will continue, abort, or pause, respectively. Experiment An Experiment is limited run of one or more ReplicaSets for the purposes of analysis. Experiments typically run for a pre-determined duration, but can also run indefinitely until stopped. Experiments may reference an AnalysisTemplate to run during or after the experiment. The canonical use case for an Experiment is to start a baseline and canary deployment in parallel, and compare the metrics produced by the baseline and canary pods for an equal comparison. Background Analysis Analysis can be run in the background -- while the canary is progressing through its rollout steps. The following example gradually increments the canary weight by 20% every 10 minutes until it reaches 100%. In the background, an AnalysisRun is started based on the AnalysisTemplate named success-rate . The success-rate template queries a prometheus server, measuring the HTTP success rates at 5 minute intervals/samples. It has no end time, and continues until stopped or failed. If the metric is measured to be less than 95%, and there are three such measurements, the analysis is considered Failed. The failed analysis causes the Rollout to abort, setting the canary weight back to zero, and the Rollout would be considered in a Degraded . Otherwise, if the rollout completes all of its canary steps, the rollout is considered successful and the analysis run is stopped by the controller. This example highlights: background analysis style of progressive delivery using a prometheus query to perform a measurement the ability to parameterize the analysis Delay starting the analysis run until step 3 (Set Weight 40%) apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : analysis : templateName : success-rate startingStep : 2 # delay starting analysis run # until setWeight: 40% args : - name : service-name value : guestbook-svc.default.svc.cluster.local steps : - setWeight : 20 - pause : { duration : 600 } - setWeight : 40 - pause : { duration : 600 } - setWeight : 60 - pause : { duration : 600 } - setWeight : 80 - pause : { duration : 600 } apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m successCondition : result = 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter= source ,destination_service=~ {{args.service-name}} ,response_code!~ 5.* }[5m] )) / sum(irate( istio_requests_total{reporter= source ,destination_service=~ {{args.service-name}} }[5m] )) analysis using wavefront query apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m successCondition : result = 0.95 failureLimit : 3 provider : wavefront : address : example.wavefront.com query : | sum(rate( 5m, ts( istio.requestcount.count , response_code!=500 and destination_service= {{args.service-name}} ))) / sum(rate( 5m, ts( istio.requestcount.count , reporter=client and destination_service= {{args.service-name}} ))) wavefront api tokens can be configured in a kubernetes secret in argo-rollouts namespace. apiVersion : v1 kind : Secret metadata : name : wavefront-api-tokens type : Opaque data : example1.wavefront.com : token1 example2.wavefront.com : token2 Analysis at a Predefined Step Analysis can also be performed as a rollout step as a \"analysis\" step. When analysis is performed as a step, an AnalysisRun is started when the step is reached, and blocks the rollout until the run is completed. The success or failure of the analysis run decides if the rollout will proceed to the next step, or abort the rollout completely. This example sets the canary weight to 20%, pauses for 5 minutes, then runs an analysis. If the analysis was successful, continues with rollout, otherwise aborts. This example demonstrates: the ability to invoke an analysis in-line as part of steps apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : steps : - setWeight : 20 - pause : { duration : 300 } - analysis : templateName : success-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local In this example, the AnalysisTemplate is identical to the background analysis example, but since no interval is specified, the analysis will perform a single measurement and complete. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate successCondition : result = 0.95 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter= source ,destination_service=~ {{args.service-name}} ,response_code!~ 5.* }[5m] )) / sum(irate( istio_requests_total{reporter= source ,destination_service=~ {{args.service-name}} }[5m] )) Multiple measurements can be performed over a longer duration period, by specifying the count and interval fields: metrics : - name : success-rate successCondition : result = 0.95 interval : 60s count : 5 provider : prometheus : address : http://prometheus.example.com:9090 query : ... Failure Conditions As an alternative to measuring success, failureCondition can be used to cause an analysis run to fail. The following example continually polls a prometheus server to get the total number of errors every 5 minutes, causing the analysis run to fail if 10 or more errors were encountered. metrics : - name : total-errors interval : 5m failureCondition : result = 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter= source ,destination_service=~ {{args.service-name}} ,response_code~ 5.* }[5m] )) Inconclusive Runs Analysis runs can also be considered Inconclusive , which indicates the run was neither successful, nor failed. Inconclusive runs causes a rollout to become paused at its current step. Manual intervention is then needed to either resume the rollout, or abort. One example of how analysis runs could become Inconclusive , is when a metric defines no success or failure conditions. metrics : - name : my-query provider : prometheus : address : http://prometheus.example.com:9090 query : ... Inconclusive analysis runs might also happen when both success and failure conditions are specified, but the measurement value did not meet either condition. metrics : - name : success-rate successCondition : result = 0.90 failureCondition : result 0.50 provider : prometheus : address : http://prometheus.example.com:9090 query : ... A use case for having Inconclusive analysis runs are to enable Argo Rollouts to automate the execution of analysis runs, and collect the measurement, but still allow human judgement to decide whether or not measurement value is acceptable and decide to proceed or abort. Delay Analysis Runs If the analysis run does not need to start immediately (i.e give the metric provider time to collect metrics on the canary version), Analysis Runs can delay the specific metric analysis. Each metric can be configured to have a different delay. In additional to the metric specific delays, the rollouts with background analysis can delay creating an analysis run until a certain step is reached Delaying a specific analysis metric: metrics : - name : success-rate initialDelay : 5m # Do not start this analysis until 5 minutes after the analysis run starts successCondition : result = 0.90 provider : prometheus : address : http://prometheus.example.com:9090 query : ... Delaying starting background analysis run until step 3 (Set Weight 40%): apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : analysis : templateName : success-rate startingStep : 2 steps : - setWeight : 20 - pause : { duration : 600 } - setWeight : 40 - pause : { duration : 600 } Experimentation (e.g. Mann-Whitney Analysis) Analysis can also be done as part of an Experiment. This example starts both a canary and baseline ReplicaSet. The ReplicaSets run for 1 hour, then scale down to zero. Call out to Kayenta to perform Mann-Whintney analysis against the two pods. Demonstrates ability to start a short-lived experiment and an asynchronous analysis. This example demonstrates: the ability to start an Experiment as part of rollout steps, which launches multiple ReplicaSets (e.g. baseline canary) the ability to reference and supply pod-template-hash to an AnalysisRun kayenta metrics apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : app : guestbook spec : ... strategy : canary : steps : - experiment : duration : 3600 templates : - name : baseline specRef : stable - name : canary specRef : canary analysis : templateName : mann-whitney args : - name : stable-hash valueFrom : podTemplateHash : Stable - name : canary-hash valueFrom : podTemplateHash : Latest apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : mann-whitney spec : args : - name : start-time - name : end-time - name : stable-hash - name : canary-hash metrics : - name : mann-whitney provider : kayenta : address : https://kayenta.example.com application : guestbook canaryConfigName : my-test thresholds : pass : 90 marginal : 75 scopes : - name : default controlScope : scope : app=guestbook and rollouts-pod-template-hash={{args.stable-hash}} step : 60 start : {{args.start-time}} end : {{args.end-time}} experimentScope : scope : app=guestbook and rollouts-pod-template-hash={{args.canary-hash}} step : 60 start : {{args.start-time}} end : {{args.end-time}} The above would instantiate the following experiment: apiVersion : argoproj.io/v1alpha1 kind : Experiment name : name : guestbook-6c54544bf9-0 spec : duration : 3600 templates : - name : baseline replicas : 1 spec : containers : - name : guestbook image : guesbook:v1 - name : canary replicas : 1 spec : containers : - name : guestbook image : guesbook:v2 analysis : templateName : mann-whitney args : - name : start-time value : 2019-09-14T01:40:10Z - name : end-time value : 2019-09-14T02:40:10Z In order to perform multiple kayenta runs over some time duration, the interval and count fields can be supplied. When the start and end fields are omitted from the kayenta scopes, the values will be implicitly decided as: start: if lookback: true start of analysis, otherwise current time - interval end: current time apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : mann-whitney spec : args : - name : stable-hash - name : canary-hash metrics : - name : mann-whitney provider : kayenta : address : https://kayenta.intuit.com application : guestbook canaryConfigName : my-test interval : 3600 count : 3 # loopback will cause start time value to be equal to start of analysis # lookback: true thresholds : pass : 90 marginal : 75 scopes : - name : default controlScope : scope : app=guestbook and rollouts-pod-template-hash={{args.stable-hash}} step : 60 experimentScope : scope : app=guestbook and rollouts-pod-template-hash={{args.canary-hash}} step : 60 Run experiment indefinitely Experiments can run for an indefinite duration by omitting the duration field. Indefinite experiments would be stopped externally, or through the completion of a referenced analysis. Blue-Green Automated Rollback Perform a blue-green deployment. After the cutover, run analysis. If the analysis succeeds, the rollout is successful, otherwise abort the rollout and cut traffic back over to the stable replicaset. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : blueGreen : postAnalysis : templateName : success-rate Job Metrics A Kubernetes Job can be used to run analysis. When a Job is used, the metric is considered successful if the Job completes and had an exit code of zero, otherwise it is failed. metrics : - name : test provider : job : backoffLimit : 1 spec : template : spec : containers : - name : test image : my-image:latest command : [ my-test-script , my-service.default.svc.cluster.local ] restartPolicy : Never Web Metrics A webhook can be used to call out to some external service to obtain the measurement. This example makes a HTTP GET request to some URL. The webhook response should return JSON content. metrics : - name : webmetric successCondition : true provider : web : url : http://my-server.com/api/v1/measurement?service={{ args.service-name }} timeoutSeconds : 20 # defaults to 10 seconds headers : - key : X-Measurement-Token value : {{ args.token }} jsonPath : {$.results.ok} In this example, the measurement is successful if the json response returns \"true\" for the nested ok field. { results : { ok : true , successPercent : 0.95 } } For success conditions that need to evaluate a numeric return value the asInt or asFloat functions can be used to convert the result value. metrics : - name : webmetric successCondition : asFloat(result) = 0.90 provider : web : url : http://my-server.com/api/v1/measurement?service={{ args.service-name }} headers : - key : X-Measurement-Token value : {{ args.token }} jsonPath : {$.results.successPercent}","title":"Analysis"},{"location":"features/analysis/#canary-analysis-progressive-delivery","text":"Argo Rollouts provides several ways to perform canary analysis to drive progressive delivery. This document describes how to achieve various forms of progressive delivery, varying the point in time analysis is performed, it's frequency, and occurrence.","title":"Canary Analysis &amp; Progressive Delivery"},{"location":"features/analysis/#custom-resource-definitions","text":"CRD Description Rollout A Rollout acts as a drop-in replacement for a Deployment resource. It provides additional blueGreen and canary update strategies. These strategies can create AnalysisRuns and Experiments during the update, which will progress the update, or abort it. AnalysisTemplate An AnalysisTemplate is a template spec which defines how to perform a canary analysis, such as the metrics which it should perform, its frequency, and the values which are considered successful or failed. AnalysisTemplates may be parameterized with inputs values. AnalysisRun An AnalysisRun is an instantiation of an AnalysisTemplate . AnalysisRuns are like Jobs in that they eventually complete. Completed runs are considered Successful, Failed, or Inconclusive, and the result of the run affect if the Rollout's update will continue, abort, or pause, respectively. Experiment An Experiment is limited run of one or more ReplicaSets for the purposes of analysis. Experiments typically run for a pre-determined duration, but can also run indefinitely until stopped. Experiments may reference an AnalysisTemplate to run during or after the experiment. The canonical use case for an Experiment is to start a baseline and canary deployment in parallel, and compare the metrics produced by the baseline and canary pods for an equal comparison.","title":"Custom Resource Definitions"},{"location":"features/analysis/#background-analysis","text":"Analysis can be run in the background -- while the canary is progressing through its rollout steps. The following example gradually increments the canary weight by 20% every 10 minutes until it reaches 100%. In the background, an AnalysisRun is started based on the AnalysisTemplate named success-rate . The success-rate template queries a prometheus server, measuring the HTTP success rates at 5 minute intervals/samples. It has no end time, and continues until stopped or failed. If the metric is measured to be less than 95%, and there are three such measurements, the analysis is considered Failed. The failed analysis causes the Rollout to abort, setting the canary weight back to zero, and the Rollout would be considered in a Degraded . Otherwise, if the rollout completes all of its canary steps, the rollout is considered successful and the analysis run is stopped by the controller. This example highlights: background analysis style of progressive delivery using a prometheus query to perform a measurement the ability to parameterize the analysis Delay starting the analysis run until step 3 (Set Weight 40%) apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : analysis : templateName : success-rate startingStep : 2 # delay starting analysis run # until setWeight: 40% args : - name : service-name value : guestbook-svc.default.svc.cluster.local steps : - setWeight : 20 - pause : { duration : 600 } - setWeight : 40 - pause : { duration : 600 } - setWeight : 60 - pause : { duration : 600 } - setWeight : 80 - pause : { duration : 600 } apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m successCondition : result = 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter= source ,destination_service=~ {{args.service-name}} ,response_code!~ 5.* }[5m] )) / sum(irate( istio_requests_total{reporter= source ,destination_service=~ {{args.service-name}} }[5m] )) analysis using wavefront query apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m successCondition : result = 0.95 failureLimit : 3 provider : wavefront : address : example.wavefront.com query : | sum(rate( 5m, ts( istio.requestcount.count , response_code!=500 and destination_service= {{args.service-name}} ))) / sum(rate( 5m, ts( istio.requestcount.count , reporter=client and destination_service= {{args.service-name}} ))) wavefront api tokens can be configured in a kubernetes secret in argo-rollouts namespace. apiVersion : v1 kind : Secret metadata : name : wavefront-api-tokens type : Opaque data : example1.wavefront.com : token1 example2.wavefront.com : token2","title":"Background Analysis"},{"location":"features/analysis/#analysis-at-a-predefined-step","text":"Analysis can also be performed as a rollout step as a \"analysis\" step. When analysis is performed as a step, an AnalysisRun is started when the step is reached, and blocks the rollout until the run is completed. The success or failure of the analysis run decides if the rollout will proceed to the next step, or abort the rollout completely. This example sets the canary weight to 20%, pauses for 5 minutes, then runs an analysis. If the analysis was successful, continues with rollout, otherwise aborts. This example demonstrates: the ability to invoke an analysis in-line as part of steps apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : steps : - setWeight : 20 - pause : { duration : 300 } - analysis : templateName : success-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local In this example, the AnalysisTemplate is identical to the background analysis example, but since no interval is specified, the analysis will perform a single measurement and complete. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate successCondition : result = 0.95 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter= source ,destination_service=~ {{args.service-name}} ,response_code!~ 5.* }[5m] )) / sum(irate( istio_requests_total{reporter= source ,destination_service=~ {{args.service-name}} }[5m] )) Multiple measurements can be performed over a longer duration period, by specifying the count and interval fields: metrics : - name : success-rate successCondition : result = 0.95 interval : 60s count : 5 provider : prometheus : address : http://prometheus.example.com:9090 query : ...","title":"Analysis at a Predefined Step"},{"location":"features/analysis/#failure-conditions","text":"As an alternative to measuring success, failureCondition can be used to cause an analysis run to fail. The following example continually polls a prometheus server to get the total number of errors every 5 minutes, causing the analysis run to fail if 10 or more errors were encountered. metrics : - name : total-errors interval : 5m failureCondition : result = 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter= source ,destination_service=~ {{args.service-name}} ,response_code~ 5.* }[5m] ))","title":"Failure Conditions"},{"location":"features/analysis/#inconclusive-runs","text":"Analysis runs can also be considered Inconclusive , which indicates the run was neither successful, nor failed. Inconclusive runs causes a rollout to become paused at its current step. Manual intervention is then needed to either resume the rollout, or abort. One example of how analysis runs could become Inconclusive , is when a metric defines no success or failure conditions. metrics : - name : my-query provider : prometheus : address : http://prometheus.example.com:9090 query : ... Inconclusive analysis runs might also happen when both success and failure conditions are specified, but the measurement value did not meet either condition. metrics : - name : success-rate successCondition : result = 0.90 failureCondition : result 0.50 provider : prometheus : address : http://prometheus.example.com:9090 query : ... A use case for having Inconclusive analysis runs are to enable Argo Rollouts to automate the execution of analysis runs, and collect the measurement, but still allow human judgement to decide whether or not measurement value is acceptable and decide to proceed or abort.","title":"Inconclusive Runs"},{"location":"features/analysis/#delay-analysis-runs","text":"If the analysis run does not need to start immediately (i.e give the metric provider time to collect metrics on the canary version), Analysis Runs can delay the specific metric analysis. Each metric can be configured to have a different delay. In additional to the metric specific delays, the rollouts with background analysis can delay creating an analysis run until a certain step is reached Delaying a specific analysis metric: metrics : - name : success-rate initialDelay : 5m # Do not start this analysis until 5 minutes after the analysis run starts successCondition : result = 0.90 provider : prometheus : address : http://prometheus.example.com:9090 query : ... Delaying starting background analysis run until step 3 (Set Weight 40%): apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : analysis : templateName : success-rate startingStep : 2 steps : - setWeight : 20 - pause : { duration : 600 } - setWeight : 40 - pause : { duration : 600 }","title":"Delay Analysis Runs"},{"location":"features/analysis/#experimentation-eg-mann-whitney-analysis","text":"Analysis can also be done as part of an Experiment. This example starts both a canary and baseline ReplicaSet. The ReplicaSets run for 1 hour, then scale down to zero. Call out to Kayenta to perform Mann-Whintney analysis against the two pods. Demonstrates ability to start a short-lived experiment and an asynchronous analysis. This example demonstrates: the ability to start an Experiment as part of rollout steps, which launches multiple ReplicaSets (e.g. baseline canary) the ability to reference and supply pod-template-hash to an AnalysisRun kayenta metrics apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : app : guestbook spec : ... strategy : canary : steps : - experiment : duration : 3600 templates : - name : baseline specRef : stable - name : canary specRef : canary analysis : templateName : mann-whitney args : - name : stable-hash valueFrom : podTemplateHash : Stable - name : canary-hash valueFrom : podTemplateHash : Latest apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : mann-whitney spec : args : - name : start-time - name : end-time - name : stable-hash - name : canary-hash metrics : - name : mann-whitney provider : kayenta : address : https://kayenta.example.com application : guestbook canaryConfigName : my-test thresholds : pass : 90 marginal : 75 scopes : - name : default controlScope : scope : app=guestbook and rollouts-pod-template-hash={{args.stable-hash}} step : 60 start : {{args.start-time}} end : {{args.end-time}} experimentScope : scope : app=guestbook and rollouts-pod-template-hash={{args.canary-hash}} step : 60 start : {{args.start-time}} end : {{args.end-time}} The above would instantiate the following experiment: apiVersion : argoproj.io/v1alpha1 kind : Experiment name : name : guestbook-6c54544bf9-0 spec : duration : 3600 templates : - name : baseline replicas : 1 spec : containers : - name : guestbook image : guesbook:v1 - name : canary replicas : 1 spec : containers : - name : guestbook image : guesbook:v2 analysis : templateName : mann-whitney args : - name : start-time value : 2019-09-14T01:40:10Z - name : end-time value : 2019-09-14T02:40:10Z In order to perform multiple kayenta runs over some time duration, the interval and count fields can be supplied. When the start and end fields are omitted from the kayenta scopes, the values will be implicitly decided as: start: if lookback: true start of analysis, otherwise current time - interval end: current time apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : mann-whitney spec : args : - name : stable-hash - name : canary-hash metrics : - name : mann-whitney provider : kayenta : address : https://kayenta.intuit.com application : guestbook canaryConfigName : my-test interval : 3600 count : 3 # loopback will cause start time value to be equal to start of analysis # lookback: true thresholds : pass : 90 marginal : 75 scopes : - name : default controlScope : scope : app=guestbook and rollouts-pod-template-hash={{args.stable-hash}} step : 60 experimentScope : scope : app=guestbook and rollouts-pod-template-hash={{args.canary-hash}} step : 60","title":"Experimentation (e.g. Mann-Whitney Analysis)"},{"location":"features/analysis/#run-experiment-indefinitely","text":"Experiments can run for an indefinite duration by omitting the duration field. Indefinite experiments would be stopped externally, or through the completion of a referenced analysis.","title":"Run experiment indefinitely"},{"location":"features/analysis/#blue-green-automated-rollback","text":"Perform a blue-green deployment. After the cutover, run analysis. If the analysis succeeds, the rollout is successful, otherwise abort the rollout and cut traffic back over to the stable replicaset. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : blueGreen : postAnalysis : templateName : success-rate","title":"Blue-Green Automated Rollback"},{"location":"features/analysis/#job-metrics","text":"A Kubernetes Job can be used to run analysis. When a Job is used, the metric is considered successful if the Job completes and had an exit code of zero, otherwise it is failed. metrics : - name : test provider : job : backoffLimit : 1 spec : template : spec : containers : - name : test image : my-image:latest command : [ my-test-script , my-service.default.svc.cluster.local ] restartPolicy : Never","title":"Job Metrics"},{"location":"features/analysis/#web-metrics","text":"A webhook can be used to call out to some external service to obtain the measurement. This example makes a HTTP GET request to some URL. The webhook response should return JSON content. metrics : - name : webmetric successCondition : true provider : web : url : http://my-server.com/api/v1/measurement?service={{ args.service-name }} timeoutSeconds : 20 # defaults to 10 seconds headers : - key : X-Measurement-Token value : {{ args.token }} jsonPath : {$.results.ok} In this example, the measurement is successful if the json response returns \"true\" for the nested ok field. { results : { ok : true , successPercent : 0.95 } } For success conditions that need to evaluate a numeric return value the asInt or asFloat functions can be used to convert the result value. metrics : - name : webmetric successCondition : asFloat(result) = 0.90 provider : web : url : http://my-server.com/api/v1/measurement?service={{ args.service-name }} headers : - key : X-Measurement-Token value : {{ args.token }} jsonPath : {$.results.successPercent}","title":"Web Metrics"},{"location":"features/bluegreen/","text":"BlueGreen Deployment Strategy A Blue Green Deployment allows users to reduce the amount of time multiple versions running at the same time. Overview In addition to managing ReplicaSets, the rollout controller will modify a Service resource during the BlueGreenUpdate strategy. The Rollout spec has users specify a reference to active service and optionally a preview service in the same namespace. The active Service is used to send regular application traffic to the old version, while the preview Service is used as funnel traffic to the new version. The rollout controller ensures proper traffic routing by injecting a unique hash of the ReplicaSet to these services' selectors. This allows the rollout to define an active and preview stack and a process to migrate replica sets from the preview to the active. When there is a change to the .spec.template field of a rollout, the controller will create the new ReplicaSet. If the active service is not sending traffic to a ReplicaSet, the controller will immediately start sending traffic to the ReplicaSet. Otherwise, the active service will point at the old ReplicaSet while the ReplicaSet becomes available. Once the new ReplicaSet becomes available, the controller will modify the active service to point at the new ReplicaSet. After waiting some time configured by the .spec.strategy.blueGreen.scaleDownDelaySeconds , the controller will scale down the old ReplicaSet. Important note When the rollout changes the selector on a service, there is a propagation delay before all the nodes update their IP tables to send traffic to the new pods instead of the old. During this delay, traffic will be directed to the old pods if the nodes have not been updated yet. In order to prevent the packets from being sent to a node that killed the old pod, the rollout uses the scaleDownDelaySeconds field to give nodes enough time to broadcast the IP table changes. Configurable Features Here are the optional fields that will change the behavior of BlueGreen deployment: spec : strategy : blueGreen : previewService : string previewReplicaCount : *int32 autoPromotionEnabled : true autoPromotionSeconds : *int32 scaleDownDelaySeconds : *int32 scaleDownDelayRevisionLimit : *int32 PreviewService The PreviewService field references a Service that will be modified to send traffic to the new replicaset before the new one is promoted to receiving traffic from the active service. Once the new replicaset start receives traffic from the active service, the preview service will be modified to send traffic to no ReplicaSets. The Rollout always makes sure that the preview service is sending traffic to the new ReplicaSet. As a result, if a new version is introduced before the old version is promoted to the active service, the controller will immediately switch over to the new version. This feature is used to provide an endpoint that can be used to test a new version of an application. Defaults to an empty string PreviewReplicaCount The PreviewReplicaCount will indicate the number of replicas that the new version of an application should run. Once the application is ready to promote to the active service, the controller will scale the new ReplicaSet to the value of the spec.replicas . The rollout will not switch over the active service to the new ReplicaSet until it matches the spec.replicas count. This feature is mainly used to save resources during the testing phase. If the application does not need a fully scaled up application for the tests, this feature can help save some resources. Defaults to nil AutoPromotionEnabled The AutoPromotionEnabled will make the rollout automatically promote the new ReplicaSet to the active service once the new ReplicaSet is healthy. This field is defaulted to true if it is not specified. Defaults to true AutoPromotionSeconds The AutoPromotionSeconds will make the rollout automatically promote the new ReplicaSet to active Service after the AutoPromotionSeconds time has passed since the rollout has entered a paused state. If the AutoPromotionEnabled field is set to true, this field will be ignored Defaults to nil ScaleDownDelaySeconds The ScaleDownDelaySeconds is used to delay scaling down the old ReplicaSet after the active Service is switched to the new ReplicaSet. Defaults to 30 ScaleDownDelayRevisionLimit The ScaleDownDelayRevisionLimit limits the number of old active ReplicaSets to keep scaled up while they wait for the scaleDownDelay to pass after being removed from the active service. Default to nil","title":"BlueGreen"},{"location":"features/bluegreen/#bluegreen-deployment-strategy","text":"A Blue Green Deployment allows users to reduce the amount of time multiple versions running at the same time.","title":"BlueGreen Deployment Strategy"},{"location":"features/bluegreen/#overview","text":"In addition to managing ReplicaSets, the rollout controller will modify a Service resource during the BlueGreenUpdate strategy. The Rollout spec has users specify a reference to active service and optionally a preview service in the same namespace. The active Service is used to send regular application traffic to the old version, while the preview Service is used as funnel traffic to the new version. The rollout controller ensures proper traffic routing by injecting a unique hash of the ReplicaSet to these services' selectors. This allows the rollout to define an active and preview stack and a process to migrate replica sets from the preview to the active. When there is a change to the .spec.template field of a rollout, the controller will create the new ReplicaSet. If the active service is not sending traffic to a ReplicaSet, the controller will immediately start sending traffic to the ReplicaSet. Otherwise, the active service will point at the old ReplicaSet while the ReplicaSet becomes available. Once the new ReplicaSet becomes available, the controller will modify the active service to point at the new ReplicaSet. After waiting some time configured by the .spec.strategy.blueGreen.scaleDownDelaySeconds , the controller will scale down the old ReplicaSet. Important note When the rollout changes the selector on a service, there is a propagation delay before all the nodes update their IP tables to send traffic to the new pods instead of the old. During this delay, traffic will be directed to the old pods if the nodes have not been updated yet. In order to prevent the packets from being sent to a node that killed the old pod, the rollout uses the scaleDownDelaySeconds field to give nodes enough time to broadcast the IP table changes.","title":"Overview"},{"location":"features/bluegreen/#configurable-features","text":"Here are the optional fields that will change the behavior of BlueGreen deployment: spec : strategy : blueGreen : previewService : string previewReplicaCount : *int32 autoPromotionEnabled : true autoPromotionSeconds : *int32 scaleDownDelaySeconds : *int32 scaleDownDelayRevisionLimit : *int32","title":"Configurable Features"},{"location":"features/bluegreen/#previewservice","text":"The PreviewService field references a Service that will be modified to send traffic to the new replicaset before the new one is promoted to receiving traffic from the active service. Once the new replicaset start receives traffic from the active service, the preview service will be modified to send traffic to no ReplicaSets. The Rollout always makes sure that the preview service is sending traffic to the new ReplicaSet. As a result, if a new version is introduced before the old version is promoted to the active service, the controller will immediately switch over to the new version. This feature is used to provide an endpoint that can be used to test a new version of an application. Defaults to an empty string","title":"PreviewService"},{"location":"features/bluegreen/#previewreplicacount","text":"The PreviewReplicaCount will indicate the number of replicas that the new version of an application should run. Once the application is ready to promote to the active service, the controller will scale the new ReplicaSet to the value of the spec.replicas . The rollout will not switch over the active service to the new ReplicaSet until it matches the spec.replicas count. This feature is mainly used to save resources during the testing phase. If the application does not need a fully scaled up application for the tests, this feature can help save some resources. Defaults to nil","title":"PreviewReplicaCount"},{"location":"features/bluegreen/#autopromotionenabled","text":"The AutoPromotionEnabled will make the rollout automatically promote the new ReplicaSet to the active service once the new ReplicaSet is healthy. This field is defaulted to true if it is not specified. Defaults to true","title":"AutoPromotionEnabled"},{"location":"features/bluegreen/#autopromotionseconds","text":"The AutoPromotionSeconds will make the rollout automatically promote the new ReplicaSet to active Service after the AutoPromotionSeconds time has passed since the rollout has entered a paused state. If the AutoPromotionEnabled field is set to true, this field will be ignored Defaults to nil","title":"AutoPromotionSeconds"},{"location":"features/bluegreen/#scaledowndelayseconds","text":"The ScaleDownDelaySeconds is used to delay scaling down the old ReplicaSet after the active Service is switched to the new ReplicaSet. Defaults to 30","title":"ScaleDownDelaySeconds"},{"location":"features/bluegreen/#scaledowndelayrevisionlimit","text":"The ScaleDownDelayRevisionLimit limits the number of old active ReplicaSets to keep scaled up while they wait for the scaleDownDelay to pass after being removed from the active service. Default to nil","title":"ScaleDownDelayRevisionLimit"},{"location":"features/canary/","text":"Canary Deployment Strategy A canary rollout is a deployment strategy where the operator releases a new version of their application to a small percentage of the production traffic. Overview Since there is no agreed upon standard for a canary deployment, the rollouts controller allows users to outline how they want to run their canary deployment. Users can define a list of steps the controller uses to manipulate the RepliaSets where there is a change to the .spec.template . Each step will be evaluated before the new ReplicaSet is promoted to the stable version, and the old version is completely scaled down. Each step can have one of two fields. The setWeight field dictates the percentage of traffic that should be sent to the canary, and the pause struct instructs the rollout to pause. When the controller reaches a pause step for a rollout, it will set the .spec.paused field to true . If the duration field within the pause struct is set, the rollout will not progress to the next step until it has waited for the value of the duration field. Otherwise, the rollout will wait indefinitely until the .spec.paused field is set to false . By using the setWeight and the pause fields, a user can declarative describe how they want to progress to the new version. Below is an example of a canary strategy. Example apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : example-rollout spec : replicas : 10 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.15.4 ports : - containerPort : 80 minReadySeconds : 30 revisionHistoryLimit : 3 strategy : canary : #Indicates that the rollout should use the Canary strategy maxSurge : 25% maxUnavailable : 0, steps : - setWeight : 10 - pause : duration : 3600 # 1 hour - setWeight : 20 - pause : {} Mimicking Rolling Update If the steps field is omitted, the canary strategy will mimic the rolling update behavior. Similar to the deployment, the canary strategy has the maxSurge and maxUnavailable fields to configure how the Rollout should progress to the new version. Other Configurable Features Here are the optional fields that will modify the behavior of canary strategy: spec : strategy : canary : maxSurge : stringOrInt maxUnavailable : stringOrInt canaryService : string maxSurge maxSurge defines the maximum number of replicas the rollout can create to move to the correct ratio set by the last setWeight. Max Surge can either be an integer or percentage as a string (i.e. \"20%\") Defaults to \"25%\". maxUnavailable The maximum number of pods that can be unavailable during the update. Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%). This can not be 0 if MaxSurge is 0. // Defaults to 0 CanaryService canaryService references a Service that will be modified to send traffic to only the canary ReplicaSet. This allows users to only hit the canary ReplicaSet. Defaults to an empty string","title":"Canary"},{"location":"features/canary/#canary-deployment-strategy","text":"A canary rollout is a deployment strategy where the operator releases a new version of their application to a small percentage of the production traffic.","title":"Canary Deployment Strategy"},{"location":"features/canary/#overview","text":"Since there is no agreed upon standard for a canary deployment, the rollouts controller allows users to outline how they want to run their canary deployment. Users can define a list of steps the controller uses to manipulate the RepliaSets where there is a change to the .spec.template . Each step will be evaluated before the new ReplicaSet is promoted to the stable version, and the old version is completely scaled down. Each step can have one of two fields. The setWeight field dictates the percentage of traffic that should be sent to the canary, and the pause struct instructs the rollout to pause. When the controller reaches a pause step for a rollout, it will set the .spec.paused field to true . If the duration field within the pause struct is set, the rollout will not progress to the next step until it has waited for the value of the duration field. Otherwise, the rollout will wait indefinitely until the .spec.paused field is set to false . By using the setWeight and the pause fields, a user can declarative describe how they want to progress to the new version. Below is an example of a canary strategy.","title":"Overview"},{"location":"features/canary/#example","text":"apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : example-rollout spec : replicas : 10 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.15.4 ports : - containerPort : 80 minReadySeconds : 30 revisionHistoryLimit : 3 strategy : canary : #Indicates that the rollout should use the Canary strategy maxSurge : 25% maxUnavailable : 0, steps : - setWeight : 10 - pause : duration : 3600 # 1 hour - setWeight : 20 - pause : {}","title":"Example"},{"location":"features/canary/#mimicking-rolling-update","text":"If the steps field is omitted, the canary strategy will mimic the rolling update behavior. Similar to the deployment, the canary strategy has the maxSurge and maxUnavailable fields to configure how the Rollout should progress to the new version.","title":"Mimicking Rolling Update"},{"location":"features/canary/#other-configurable-features","text":"Here are the optional fields that will modify the behavior of canary strategy: spec : strategy : canary : maxSurge : stringOrInt maxUnavailable : stringOrInt canaryService : string","title":"Other Configurable Features"},{"location":"features/canary/#maxsurge","text":"maxSurge defines the maximum number of replicas the rollout can create to move to the correct ratio set by the last setWeight. Max Surge can either be an integer or percentage as a string (i.e. \"20%\") Defaults to \"25%\".","title":"maxSurge"},{"location":"features/canary/#maxunavailable","text":"The maximum number of pods that can be unavailable during the update. Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%). This can not be 0 if MaxSurge is 0. // Defaults to 0","title":"maxUnavailable"},{"location":"features/canary/#canaryservice","text":"canaryService references a Service that will be modified to send traffic to only the canary ReplicaSet. This allows users to only hit the canary ReplicaSet. Defaults to an empty string","title":"CanaryService"},{"location":"features/controller-metrics/","text":"","title":"Controller Metrics"},{"location":"features/experiment/","text":"Experiment CRD What is the Experiment CRD? The Experiment CRD allows users to have ephemeral runs of one or more ReplicaSets. In addition to running ephemeral ReplicaSets, the Experiment CRD can launch AnalysisRuns alongside the ReplicaSets. Generally, those AnalysisRun is used to confirm that new ReplicaSets are running as expected. Use cases of the Experiment CRD A user wants to run two versions of an application for a specific duration to enable Kayenta-style analysis of their application. The Experiment CRD creates 2 ReplicaSets (a baseline and a canary) based on the spec.templates field of the Experiment and waits until both are healthy. After the duration passes, the Experiment scales down the ReplicaSets, and the user can start the Kayenta analysis run. A user can use experiments to enable A/B/C testing by launching multiple experiments with a different version of their application for a long duration. Each Experiment has one PodSpec template that defines a specific version a user would want to run. The Experiment allows users to launch multiple experiments at once and keep each Experiment self-contained. Launching a new version of an existing application with different labels to avoid receiving traffic from a Kubernetes service. The user can run tests against the new version before continuing the Rollout. Experiment Spec Below is an example of an experiment that creates two ReplicaSets with 1 replica each and runs them for 60 seconds once they both become available. Also, the controller launches two AnalysisRuns after the ReplicaSets become available. apiVersion : argoproj.io/v1alpha1 kind : Experiment metadata : name : example-experiment spec : duration : 60 # How long to run the Experiment once the ReplicaSets created from the templates are healthy progressDeadlineSeconds : 30 templates : - name : purple # (required) Unique name for the template that gets used as a part of the ReplicaSet name. replicas : 1 selector : # Same selector that has been as in Deployments and Rollouts matchLabels : app : canary-demo color : purple template : metadata : labels : app : canary-demo color : purple spec : # Same Pod Spec that has been as in Deployments and Rollouts containers : - name : rollouts-demo image : argoproj/rollouts-demo:purple imagePullPolicy : Always ports : - name : http containerPort : 8080 protocol : TCP - name : orange replicas : 1 minReadySeconds : 10 selector : # Same selector that has been as in Deployments and Rollouts matchLabels : app : canary-demo color : orange template : metadata : labels : app : canary-demo color : orange spec : # Same Pod Spec that has been as in Deployments and Rollouts containers : - name : rollouts-demo image : argoproj/rollouts-demo:orange imagePullPolicy : Always ports : - name : http containerPort : 8080 protocol : TCP analyses : - name : http-benchmarkple templateName : http-benchmark args : - name : host value : purple - name : orange templateName : http-benchmark args : - name : host value : orange How does it work? The Experiment controller has two primary responsibilities for each Experiment: Creating and scaling ReplicaSets Creating and watching AnalysisRuns The controller creates a ReplicaSet for each template in the Experiment's .spec.templates . Each template needs a unique name as the controller generates the ReplicaSet's names from the combination of the Experiment's name and template's name. Once the controller creates the ReplicaSets, it waits until those new ReplicaSets become available. Once all the ReplicaSets are available, the controller marks the Experiment as running. The Experiment stays in this state for the duration listed in the spec.duration field or indefinitely if omitted. Once the Experiment is running, the controller creates AnalysisRuns for each analysis listed in the Experiment's .spec.analysis field. These AnalysisRun execute in parallel with the running ReplicaSets. The controller generates the AnalysisRun's name by combining the experiment name and the analysis name with a dash. If an AnalysisRun exists with that name, the controller appends a number to the generated name before recreating the AnalysisRun. If there is another collision, the controller increments the number and try again until it creates an AnalysisRun. Once the Experiment finished, the controller scales down the ReplicaSets it created and terminates the AnalysisRuns if they have not finished. An Experiment is considered complete when: More than the spec.Duration amount of time has passed since the ReplicaSets became healthy. One of the ReplicaSets does not become available, and the progress deadline seconds pass. An AnalysisRun created by an Experiment enters a failed or error state. An external process (i.e. user or pipeline) sets the .spec.terminate to true Integration With Rollouts A rollout using the Canary strategy can create an experiment using the experiment step. The experiment step serves a blocking step for the Rollout as the Rollout does not continue until the Experiment succeeds. The Rollout creates an Experiment using the configuration in the experiment step of the Rollout. The controller generates the Experiment's name by combining the Rollout's name, the PodHash of the new ReplicaSet, the current revision of the Rollout, and the current step-index. If the Experiment fails or errors out, the Rollout enters an aborted state. While in the aborted state, the Rollout fully scales up the stable version and resets the current step index back to zero. Here is an example of a rollout with an experiment step: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : app : guestbook spec : strategy : canary : steps : - experiment : duration : 3600 templates : - name : baseline specRef : stable replicas : 3 # optional defaults to 1 - name : canary specRef : canary analysis : templateName : mann-whitney args : - name : stable-hash valueFrom : podTemplateHash : baseline - name : canary-hash valueFrom : podTemplateHash : canary In the example above, the Experiment has two templates. The baseline template uses the PodSpec from the stable ReplicaSet, and the canary template uses the PodSpec from the canary ReplicaSet. The Experiment also has one analysis with the mann-whitney template. The stable-hash arg grabs the PodHash from the stable ReplicasSet, and the canary-hash arg grabs the PodHash from the canary ReplicasSet.","title":"Experiments"},{"location":"features/experiment/#experiment-crd","text":"","title":"Experiment CRD"},{"location":"features/experiment/#what-is-the-experiment-crd","text":"The Experiment CRD allows users to have ephemeral runs of one or more ReplicaSets. In addition to running ephemeral ReplicaSets, the Experiment CRD can launch AnalysisRuns alongside the ReplicaSets. Generally, those AnalysisRun is used to confirm that new ReplicaSets are running as expected.","title":"What is the Experiment CRD?"},{"location":"features/experiment/#use-cases-of-the-experiment-crd","text":"A user wants to run two versions of an application for a specific duration to enable Kayenta-style analysis of their application. The Experiment CRD creates 2 ReplicaSets (a baseline and a canary) based on the spec.templates field of the Experiment and waits until both are healthy. After the duration passes, the Experiment scales down the ReplicaSets, and the user can start the Kayenta analysis run. A user can use experiments to enable A/B/C testing by launching multiple experiments with a different version of their application for a long duration. Each Experiment has one PodSpec template that defines a specific version a user would want to run. The Experiment allows users to launch multiple experiments at once and keep each Experiment self-contained. Launching a new version of an existing application with different labels to avoid receiving traffic from a Kubernetes service. The user can run tests against the new version before continuing the Rollout.","title":"Use cases of the Experiment CRD"},{"location":"features/experiment/#experiment-spec","text":"Below is an example of an experiment that creates two ReplicaSets with 1 replica each and runs them for 60 seconds once they both become available. Also, the controller launches two AnalysisRuns after the ReplicaSets become available. apiVersion : argoproj.io/v1alpha1 kind : Experiment metadata : name : example-experiment spec : duration : 60 # How long to run the Experiment once the ReplicaSets created from the templates are healthy progressDeadlineSeconds : 30 templates : - name : purple # (required) Unique name for the template that gets used as a part of the ReplicaSet name. replicas : 1 selector : # Same selector that has been as in Deployments and Rollouts matchLabels : app : canary-demo color : purple template : metadata : labels : app : canary-demo color : purple spec : # Same Pod Spec that has been as in Deployments and Rollouts containers : - name : rollouts-demo image : argoproj/rollouts-demo:purple imagePullPolicy : Always ports : - name : http containerPort : 8080 protocol : TCP - name : orange replicas : 1 minReadySeconds : 10 selector : # Same selector that has been as in Deployments and Rollouts matchLabels : app : canary-demo color : orange template : metadata : labels : app : canary-demo color : orange spec : # Same Pod Spec that has been as in Deployments and Rollouts containers : - name : rollouts-demo image : argoproj/rollouts-demo:orange imagePullPolicy : Always ports : - name : http containerPort : 8080 protocol : TCP analyses : - name : http-benchmarkple templateName : http-benchmark args : - name : host value : purple - name : orange templateName : http-benchmark args : - name : host value : orange","title":"Experiment Spec"},{"location":"features/experiment/#how-does-it-work","text":"The Experiment controller has two primary responsibilities for each Experiment: Creating and scaling ReplicaSets Creating and watching AnalysisRuns The controller creates a ReplicaSet for each template in the Experiment's .spec.templates . Each template needs a unique name as the controller generates the ReplicaSet's names from the combination of the Experiment's name and template's name. Once the controller creates the ReplicaSets, it waits until those new ReplicaSets become available. Once all the ReplicaSets are available, the controller marks the Experiment as running. The Experiment stays in this state for the duration listed in the spec.duration field or indefinitely if omitted. Once the Experiment is running, the controller creates AnalysisRuns for each analysis listed in the Experiment's .spec.analysis field. These AnalysisRun execute in parallel with the running ReplicaSets. The controller generates the AnalysisRun's name by combining the experiment name and the analysis name with a dash. If an AnalysisRun exists with that name, the controller appends a number to the generated name before recreating the AnalysisRun. If there is another collision, the controller increments the number and try again until it creates an AnalysisRun. Once the Experiment finished, the controller scales down the ReplicaSets it created and terminates the AnalysisRuns if they have not finished. An Experiment is considered complete when: More than the spec.Duration amount of time has passed since the ReplicaSets became healthy. One of the ReplicaSets does not become available, and the progress deadline seconds pass. An AnalysisRun created by an Experiment enters a failed or error state. An external process (i.e. user or pipeline) sets the .spec.terminate to true","title":"How does it work?"},{"location":"features/experiment/#integration-with-rollouts","text":"A rollout using the Canary strategy can create an experiment using the experiment step. The experiment step serves a blocking step for the Rollout as the Rollout does not continue until the Experiment succeeds. The Rollout creates an Experiment using the configuration in the experiment step of the Rollout. The controller generates the Experiment's name by combining the Rollout's name, the PodHash of the new ReplicaSet, the current revision of the Rollout, and the current step-index. If the Experiment fails or errors out, the Rollout enters an aborted state. While in the aborted state, the Rollout fully scales up the stable version and resets the current step index back to zero. Here is an example of a rollout with an experiment step: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : app : guestbook spec : strategy : canary : steps : - experiment : duration : 3600 templates : - name : baseline specRef : stable replicas : 3 # optional defaults to 1 - name : canary specRef : canary analysis : templateName : mann-whitney args : - name : stable-hash valueFrom : podTemplateHash : baseline - name : canary-hash valueFrom : podTemplateHash : canary In the example above, the Experiment has two templates. The baseline template uses the PodSpec from the stable ReplicaSet, and the canary template uses the PodSpec from the canary ReplicaSet. The Experiment also has one analysis with the mann-whitney template. The stable-hash arg grabs the PodHash from the stable ReplicasSet, and the canary-hash arg grabs the PodHash from the canary ReplicasSet.","title":"Integration With Rollouts"},{"location":"features/hpa-support/","text":"Horizontal Pod Autoscaling Horizontal Pod Autoscaling (HPA) automatically scales the number of pods in owned by a Kubernetes resource based on observed CPU utilization or user-configured metrics. In order to accomplish this behavior, HPA only supports resources with the scale endpoint enabled with a couple of required fields. The scale endpoint allows the HPA to understand the current state of a resource and modify the resource to scale it appropriately. Argo Rollouts added support for the scale endpoint in the 0.3.0 release. After being modified by the HPA, the Argo Rollouts controller is responsible for reconciling that change in replicas. Since the strategies within a Rollout are very different, the Argo Rollouts controller handles the scale endpoint differently for various strategies. Below is the behavior for the different strategies: Blue Green The HPA will scale rollouts using the BlueGreen strategy using the metrics from the ReplicaSet receiving traffic from the active service. When the HPA changes the replicas count, the Argo Rollouts controller will first scale up the ReplicaSet receiving traffic from the active service before ReplicaSet receiving traffic from the preview service. The controller will scale up the ReplicaSet receiving traffic from the preview service to prepare it for when the rollout switches the preview to active. If there are no ReplicaSets receiving from the active service, the controller will use all the pods that match the base selector to determine scaling events. In that case, the controller will scale up the latest ReplicaSet to the new count and scale down the older ReplicaSets. Canary (ReplicaSet based) The HPA will scale rollouts using the Canary Strategy using the metrics of all the ReplicasSets within the rollout. Since the Argo Rollouts controller does not control the service that sends traffic to those ReplicaSets, it assumes that all the ReplicaSets in the rollout are receiving traffic. Example Below is an example of a Horizontal Pod Autoscaler that scales a rollout based on CPU metrics: apiVersion : autoscaling/v1 kind : HorizontalPodAutoscaler metadata : name : hpa-rollout-example spec : maxReplicas : 6 minReplicas : 2 scaleTargetRef : apiVersion : argoproj.io kind : Rollout name : example-rollout targetCPUUtilizationPercentage : 80 Requirements In order for the HPA to manipulate the rollout, the Kubernetes cluster hosting the rollout CRD needs the subresources support for CRDs. This feature was introduced as alpha in Kubernetes version 1.10 and transitioned to beta in Kubernetes version 1.11. If a user wants to use HPA on v1.10, the Kubernetes Cluster operator will need to add a custom feature flag to the API server. After 1.10, the flag is turned on by default. Check out the following link for more information on setting the custom feature flag.","title":"HPA Support"},{"location":"features/hpa-support/#horizontal-pod-autoscaling","text":"Horizontal Pod Autoscaling (HPA) automatically scales the number of pods in owned by a Kubernetes resource based on observed CPU utilization or user-configured metrics. In order to accomplish this behavior, HPA only supports resources with the scale endpoint enabled with a couple of required fields. The scale endpoint allows the HPA to understand the current state of a resource and modify the resource to scale it appropriately. Argo Rollouts added support for the scale endpoint in the 0.3.0 release. After being modified by the HPA, the Argo Rollouts controller is responsible for reconciling that change in replicas. Since the strategies within a Rollout are very different, the Argo Rollouts controller handles the scale endpoint differently for various strategies. Below is the behavior for the different strategies:","title":"Horizontal Pod Autoscaling"},{"location":"features/hpa-support/#blue-green","text":"The HPA will scale rollouts using the BlueGreen strategy using the metrics from the ReplicaSet receiving traffic from the active service. When the HPA changes the replicas count, the Argo Rollouts controller will first scale up the ReplicaSet receiving traffic from the active service before ReplicaSet receiving traffic from the preview service. The controller will scale up the ReplicaSet receiving traffic from the preview service to prepare it for when the rollout switches the preview to active. If there are no ReplicaSets receiving from the active service, the controller will use all the pods that match the base selector to determine scaling events. In that case, the controller will scale up the latest ReplicaSet to the new count and scale down the older ReplicaSets.","title":"Blue Green"},{"location":"features/hpa-support/#canary-replicaset-based","text":"The HPA will scale rollouts using the Canary Strategy using the metrics of all the ReplicasSets within the rollout. Since the Argo Rollouts controller does not control the service that sends traffic to those ReplicaSets, it assumes that all the ReplicaSets in the rollout are receiving traffic.","title":"Canary (ReplicaSet based)"},{"location":"features/hpa-support/#example","text":"Below is an example of a Horizontal Pod Autoscaler that scales a rollout based on CPU metrics: apiVersion : autoscaling/v1 kind : HorizontalPodAutoscaler metadata : name : hpa-rollout-example spec : maxReplicas : 6 minReplicas : 2 scaleTargetRef : apiVersion : argoproj.io kind : Rollout name : example-rollout targetCPUUtilizationPercentage : 80","title":"Example"},{"location":"features/hpa-support/#requirements","text":"In order for the HPA to manipulate the rollout, the Kubernetes cluster hosting the rollout CRD needs the subresources support for CRDs. This feature was introduced as alpha in Kubernetes version 1.10 and transitioned to beta in Kubernetes version 1.11. If a user wants to use HPA on v1.10, the Kubernetes Cluster operator will need to add a custom feature flag to the API server. After 1.10, the flag is turned on by default. Check out the following link for more information on setting the custom feature flag.","title":"Requirements"},{"location":"features/kubectl-plugin/","text":"Kubectl Plugin Kubectl plugins are a way to extend the kubectl command to provide additional behavior. Generally, they are used to add new functionality to kubectl and automate scriptable workflows against a cluster. The official documentation on them is here . Argo Rollouts offers a Kubectl plugin to enrich the experience with Rollouts, Experiments, and Analysis from the command line. It offers the ability to visualize the Argo Rollouts resources and run routine operations like promote or retry on those resources from the command. Installation Manual Install Argo Rollouts Kubectl plugin with curl. curl -LO https://github.com/argoproj/argo-rollouts/releases/download/v0.6.0/kubectl-argo-rollouts-darwin-amd64 Note: For Linux dist, replace darwin with linux Make the kubectl-argo-rollouts binary executable. chmod + x . / kubectl - argo - rollouts - darwin - amd64 Move the binary into your PATH. sudo mv . / kubectl - argo - rollouts - darwin - amd64 / usr / local / bin / kubectl - argo - rollouts Test to ensure the version you installed is up-to-date: kubectl argo rollouts version Krew Currently not supported, but there are plans to make the Argo Rollouts kubectl a part of Krew . Please follow this issue for the most up-to-date information. Usage The best way to get information on the available Argo Rollouts kubectl plugin commands is by run kubectl argo rollouts . The plugin lists all the available commands that the tool can execute along with a description of each commend. All the plugin's commands interact with the Kubernetes API server and use KubeConfig credentials for authentication. Since the plugin leverages the KubeConfig of the user running the command, the plugin has the permissions of those configs. Similar to kubectl, the plugin uses many of the same flags as the kubectl. For example, the kubectl argo rollouts get canary-demo -w command starts a watch on the canary-demo rollout object similar to how the kubectl get deployment canary-demo -w command starts a watch on a deployment. Visualizing Rollouts and Experiments In addition to encapsulating many routine commands, the Argo Rollouts kubectl plugin supports visualizing rollouts and experiments with the get command. The get command provides a clean representation of either the rollouts or the experiments running in a cluster. It returns a bunch of metadata on a resource and a tree view of the child resources created by the parent. As an example, here is a rollout retrieved with a get command: Here is a table to explain some of the icons on the tree view: Icon Kind \u27f3 Rollout \u03a3 Experiment \u03b1 AnalysisRun # Revision \u29c9 ReplicaSet \u25a1 Pod \u229e Job If the get command includes the watch flag ( -w or --watch ), the terminal updates as the rollouts or experiment progress highlighting the progress.","title":"Kubectl Plugin"},{"location":"features/kubectl-plugin/#kubectl-plugin","text":"Kubectl plugins are a way to extend the kubectl command to provide additional behavior. Generally, they are used to add new functionality to kubectl and automate scriptable workflows against a cluster. The official documentation on them is here . Argo Rollouts offers a Kubectl plugin to enrich the experience with Rollouts, Experiments, and Analysis from the command line. It offers the ability to visualize the Argo Rollouts resources and run routine operations like promote or retry on those resources from the command.","title":"Kubectl Plugin"},{"location":"features/kubectl-plugin/#installation","text":"","title":"Installation"},{"location":"features/kubectl-plugin/#manual","text":"Install Argo Rollouts Kubectl plugin with curl. curl -LO https://github.com/argoproj/argo-rollouts/releases/download/v0.6.0/kubectl-argo-rollouts-darwin-amd64 Note: For Linux dist, replace darwin with linux Make the kubectl-argo-rollouts binary executable. chmod + x . / kubectl - argo - rollouts - darwin - amd64 Move the binary into your PATH. sudo mv . / kubectl - argo - rollouts - darwin - amd64 / usr / local / bin / kubectl - argo - rollouts Test to ensure the version you installed is up-to-date: kubectl argo rollouts version","title":"Manual"},{"location":"features/kubectl-plugin/#krew","text":"Currently not supported, but there are plans to make the Argo Rollouts kubectl a part of Krew . Please follow this issue for the most up-to-date information.","title":"Krew"},{"location":"features/kubectl-plugin/#usage","text":"The best way to get information on the available Argo Rollouts kubectl plugin commands is by run kubectl argo rollouts . The plugin lists all the available commands that the tool can execute along with a description of each commend. All the plugin's commands interact with the Kubernetes API server and use KubeConfig credentials for authentication. Since the plugin leverages the KubeConfig of the user running the command, the plugin has the permissions of those configs. Similar to kubectl, the plugin uses many of the same flags as the kubectl. For example, the kubectl argo rollouts get canary-demo -w command starts a watch on the canary-demo rollout object similar to how the kubectl get deployment canary-demo -w command starts a watch on a deployment.","title":"Usage"},{"location":"features/kubectl-plugin/#visualizing-rollouts-and-experiments","text":"In addition to encapsulating many routine commands, the Argo Rollouts kubectl plugin supports visualizing rollouts and experiments with the get command. The get command provides a clean representation of either the rollouts or the experiments running in a cluster. It returns a bunch of metadata on a resource and a tree view of the child resources created by the parent. As an example, here is a rollout retrieved with a get command: Here is a table to explain some of the icons on the tree view: Icon Kind \u27f3 Rollout \u03a3 Experiment \u03b1 AnalysisRun # Revision \u29c9 ReplicaSet \u25a1 Pod \u229e Job If the get command includes the watch flag ( -w or --watch ), the terminal updates as the rollouts or experiment progress highlighting the progress.","title":"Visualizing Rollouts and Experiments"},{"location":"features/kustomize/","text":"Kustomize Integration Kustomize can be extended to understand CRD objects through the use of transformer configs . Using transformer configs, kustomize can be \"taught\" about the structure of a Rollout object and leverage kustomize features such as ConfigMap/Secret generators, variable references, and common labels annotations. To use Rollouts with kustomize: Download rollout-transform.yaml into your kustomize directory. Include rollout-transform.yaml in your kustomize configurations section: kind : Kustomization apiVersion : kustomize.config.k8s.io/v1beta1 configurations : - rollout-transform.yaml A example kustomize app demonstrating the ability to use transformers with Rollouts can be seen here .","title":"Kustomize Support"},{"location":"features/kustomize/#kustomize-integration","text":"Kustomize can be extended to understand CRD objects through the use of transformer configs . Using transformer configs, kustomize can be \"taught\" about the structure of a Rollout object and leverage kustomize features such as ConfigMap/Secret generators, variable references, and common labels annotations. To use Rollouts with kustomize: Download rollout-transform.yaml into your kustomize directory. Include rollout-transform.yaml in your kustomize configurations section: kind : Kustomization apiVersion : kustomize.config.k8s.io/v1beta1 configurations : - rollout-transform.yaml A example kustomize app demonstrating the ability to use transformers with Rollouts can be seen here .","title":"Kustomize Integration"},{"location":"features/traffic-management/","text":"Traffic management Traffic management is controlling the data plane to have intelligent routing rules for an application. These routing rules can manipulate the flow of traffic to different versions of an application enabling Progressive Delivery. These controls limit the blast radius of a new release by ensuring a small percentage of users receive a new version while it is verified. There are various techniques to achieve traffic management: Raw percentages (i.e., 5% of traffic should go to the new version while the rest goes to the stable version) Header-based routing (i.e., send requests with a specific header to the new version) Mirrored traffic where all the traffic is copied and send to the new version in parallel (but the response is ignored) Traffic Management tools in Kubernetes The core Kubernetes objects do not have fine-grained tools needed to fulfill all the requirements of traffic management. At most, Kubernetes offers na\u00efve load balancing capabilities through the Service object by offering an endpoint that routes traffic to a grouping of pods based on that Service's selector. Functionality like traffic mirroring or routing by headers is not possible with the default core Service object, and the only way to control the percentage of traffic to different versions of an application is by manipulating replica counts of those versions. Service Meshes fill this missing functionality in Kubernetes. They introduce new concepts and functionality to control the data plane through the use of CRDs and other core Kubernetes resources. How does Argo Rollouts enable traffic management? Argo Rollouts enables traffic management by manipulating the Service Mesh resources to match the intent of the Rollout. Argo Rollouts currently supports the following service meshes: Istio Nginx Ingress Controller File a ticket here if you would like another implementation (or thumbs up it if that issue already exists) Regardless of the Service Mesh used, the Rollout object has to set a canary Service and a stable Service in its spec. Here is an example with those fields set: apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : ... strategy : canary : canaryService : canary-service stableService : stable-service trafficRouting : ... The controller modifies these Services to route traffic to the appropriate canary and stable ReplicaSets as the Rollout progresses. These Services are used by the Service Mesh to define what group of pods should receive the canary and stable traffic. Additionally, the Argo Rollouts controller needs to treat the Rollout object differently when using traffic management. In particular, the Stable ReplicaSet owned by the Rollout remains fully scaled up as the Rollout progresses through the Canary steps. Since the traffic is controlled independently by the Service Mesh resources, the controller needs to make a best effort to ensure that the Stable and New ReplicaSets are not overwhelmed by the traffic sent to them. By leaving the Stable ReplicaSet scaled up, the controller is ensuring that the Stable ReplicaSet can handle 100% of the traffic at any time*. The New ReplicaSet follows the same behavior as without traffic management. The new ReplicaSet's replica count is equal to the latest SetWeight step percentage multiple by the total replica count of the Rollout. This calculation ensures that the canary version does not receive more traffic than it can handle. *The Rollout has to assume that the application can handle 100% of traffic if it is fully scaled up. It should outsource to the HPA to detect if the Rollout needs to more replicas if 100% isn't enough.","title":"Overview"},{"location":"features/traffic-management/#traffic-management","text":"Traffic management is controlling the data plane to have intelligent routing rules for an application. These routing rules can manipulate the flow of traffic to different versions of an application enabling Progressive Delivery. These controls limit the blast radius of a new release by ensuring a small percentage of users receive a new version while it is verified. There are various techniques to achieve traffic management: Raw percentages (i.e., 5% of traffic should go to the new version while the rest goes to the stable version) Header-based routing (i.e., send requests with a specific header to the new version) Mirrored traffic where all the traffic is copied and send to the new version in parallel (but the response is ignored)","title":"Traffic management"},{"location":"features/traffic-management/#traffic-management-tools-in-kubernetes","text":"The core Kubernetes objects do not have fine-grained tools needed to fulfill all the requirements of traffic management. At most, Kubernetes offers na\u00efve load balancing capabilities through the Service object by offering an endpoint that routes traffic to a grouping of pods based on that Service's selector. Functionality like traffic mirroring or routing by headers is not possible with the default core Service object, and the only way to control the percentage of traffic to different versions of an application is by manipulating replica counts of those versions. Service Meshes fill this missing functionality in Kubernetes. They introduce new concepts and functionality to control the data plane through the use of CRDs and other core Kubernetes resources.","title":"Traffic Management tools in Kubernetes"},{"location":"features/traffic-management/#how-does-argo-rollouts-enable-traffic-management","text":"Argo Rollouts enables traffic management by manipulating the Service Mesh resources to match the intent of the Rollout. Argo Rollouts currently supports the following service meshes: Istio Nginx Ingress Controller File a ticket here if you would like another implementation (or thumbs up it if that issue already exists) Regardless of the Service Mesh used, the Rollout object has to set a canary Service and a stable Service in its spec. Here is an example with those fields set: apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : ... strategy : canary : canaryService : canary-service stableService : stable-service trafficRouting : ... The controller modifies these Services to route traffic to the appropriate canary and stable ReplicaSets as the Rollout progresses. These Services are used by the Service Mesh to define what group of pods should receive the canary and stable traffic. Additionally, the Argo Rollouts controller needs to treat the Rollout object differently when using traffic management. In particular, the Stable ReplicaSet owned by the Rollout remains fully scaled up as the Rollout progresses through the Canary steps. Since the traffic is controlled independently by the Service Mesh resources, the controller needs to make a best effort to ensure that the Stable and New ReplicaSets are not overwhelmed by the traffic sent to them. By leaving the Stable ReplicaSet scaled up, the controller is ensuring that the Stable ReplicaSet can handle 100% of the traffic at any time*. The New ReplicaSet follows the same behavior as without traffic management. The new ReplicaSet's replica count is equal to the latest SetWeight step percentage multiple by the total replica count of the Rollout. This calculation ensures that the canary version does not receive more traffic than it can handle. *The Rollout has to assume that the application can handle 100% of traffic if it is fully scaled up. It should outsource to the HPA to detect if the Rollout needs to more replicas if 100% isn't enough.","title":"How does Argo Rollouts enable traffic management?"},{"location":"features/traffic-management/istio/","text":"Istio Istio is one of the most popular service mesh in the community and offers a rich feature-set to control the flow of traffic. Istio offers this functionality through a set of CRDs, and the Argo Rollouts controller modifies these resources to manipulate the traffic routing into the desired state. However, the Argo Rollouts controller modifies the Istio resources minimally to gives the developer flexibility while configuring their resources. Istio and Rollouts The Argo Rollout controller achieves traffic shaping by manipulating the Virtual Service. A Virtual Service provides the flexibility to define how to route traffic when a host address is hit. The Argo Rollouts controller manipulates the Virtual Service by using the following Rollout configuration: Canary Service name Stable Service name Virtual Service Name Which HTTP Routes in the Virtual Service Below is an example of a Rollout with all the required fields configured: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-example spec : ... strategy : canary : steps : - setWeight : 5 - pause : duration : 600 canaryService : canary-svc # required stableService : stable-svc # required trafficRouting : istio : virtualService : name : rollout-vsvc # required routes : - primary # At least one route is required The controller looks for both the canary and stable service listed as destination hosts within the HTTP routes of the specified Virtual Service and modifies the weights of the destinations as desired. The above Rollout expects that there is a virtual service named rollout-vsvc with an HTTP route named primary . That route should have exactly two destinations: canary-service and stable-svc Services. The canary-svc and stable-svc are required because they indicate to Istio which Pods are the Stable and Canary pods. Here is a Virtual Service that works with the Rollout specified above: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : rollout-vsvc spec : gateways : - istio-rollout-gateway hosts : - istio-rollout.dev.argoproj.io http : - name : primary route : - destination : host : stable-svc weight : 100 - destination : host : canary-svc weight : 0 When the above Rollout progresses through its steps, the controller changes Virtual Service's stable-svc 's weight to 95 and canary-svc 's to 5, wait 5 minutes, and then scales up the canary ReplicaSet to a full replica count. Once it is entirely healthy, the controller changes stable-svc 's selector to point at the canary ReplicaSet and switch the weight back to 100 to stable-svc and 0 to canary-svc . Note: The Rollout does not make any other assumptions about the fields within the Virtual Service or the Istio mesh. The user could specify additional configurations for the virtual service like URI rewrite rules on the primary route or any other route if desired. The user can also create specific destination rules for each of the services. Integrating with GitOps The above strategy introduces a problem for users practicing GitOps. The Rollout requires the user-defined Virtual Service to define an HTTP route with both destinations hosts. However, Istio requires routes with multiple destinations to assign a weight to each destination. Since the Argo Rollout controller modifies these Virtual Service's weights as a Rollout progresses through its steps, the Virtual Service becomes out of sync with the Git version. Additionally, if a GitOps tool does an apply after the Argo Rollouts controller changes the Virtual Service's weight, the apply would revert the weight to the percentage stored in the Git repo. At best, the user can specify the desired weight of 100% to the stable service and 0% to the canary service. In this case, the Virtual Service is synced with the Git repo when the Rollout completed all the steps. Argo CD has an open issue here to address this problem. The proposed solution is to introduce an annotation to the VirtualService which tells Argo CD controller to respect the current weights listed and let the Argo Rollouts controller manage them instead. Alternatives Considered Have the Rollout Own the Virtual Service Instead of the controller modifying a reference to a Virtual Service, the Rollout controller would create, manage, and own a Virtual Service. While this approach is GitOps friendly, it introduces other issues: * To provide the same flexibility as referencing Virtual Service within a Rollout, the Rollout needs to inline a large portion of the Istio spec. However, networking is outside the responsibility of the Rollout and makes the Rollout spec necessary complicated. * If Istio introduces a feature, that feature will not be available in Argo Rollouts until implemented within Argo Rollouts. Both of these issues adds more complexity to the users and Argo Rollouts developers compared to referencing a Virtual Service. Implement Istio support through the SMI SMI is the Service Mesh Interface, which serves as a standard interface for all common features of a service mesh. This feature is GitOps friendly, but native Istio has extra functionality that SMI does not currently provide. Granted, Argo Rollouts should integrate with the SMI independent of the native Istio integration.","title":"Istio"},{"location":"features/traffic-management/istio/#istio","text":"Istio is one of the most popular service mesh in the community and offers a rich feature-set to control the flow of traffic. Istio offers this functionality through a set of CRDs, and the Argo Rollouts controller modifies these resources to manipulate the traffic routing into the desired state. However, the Argo Rollouts controller modifies the Istio resources minimally to gives the developer flexibility while configuring their resources.","title":"Istio"},{"location":"features/traffic-management/istio/#istio-and-rollouts","text":"The Argo Rollout controller achieves traffic shaping by manipulating the Virtual Service. A Virtual Service provides the flexibility to define how to route traffic when a host address is hit. The Argo Rollouts controller manipulates the Virtual Service by using the following Rollout configuration: Canary Service name Stable Service name Virtual Service Name Which HTTP Routes in the Virtual Service Below is an example of a Rollout with all the required fields configured: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-example spec : ... strategy : canary : steps : - setWeight : 5 - pause : duration : 600 canaryService : canary-svc # required stableService : stable-svc # required trafficRouting : istio : virtualService : name : rollout-vsvc # required routes : - primary # At least one route is required The controller looks for both the canary and stable service listed as destination hosts within the HTTP routes of the specified Virtual Service and modifies the weights of the destinations as desired. The above Rollout expects that there is a virtual service named rollout-vsvc with an HTTP route named primary . That route should have exactly two destinations: canary-service and stable-svc Services. The canary-svc and stable-svc are required because they indicate to Istio which Pods are the Stable and Canary pods. Here is a Virtual Service that works with the Rollout specified above: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : rollout-vsvc spec : gateways : - istio-rollout-gateway hosts : - istio-rollout.dev.argoproj.io http : - name : primary route : - destination : host : stable-svc weight : 100 - destination : host : canary-svc weight : 0 When the above Rollout progresses through its steps, the controller changes Virtual Service's stable-svc 's weight to 95 and canary-svc 's to 5, wait 5 minutes, and then scales up the canary ReplicaSet to a full replica count. Once it is entirely healthy, the controller changes stable-svc 's selector to point at the canary ReplicaSet and switch the weight back to 100 to stable-svc and 0 to canary-svc . Note: The Rollout does not make any other assumptions about the fields within the Virtual Service or the Istio mesh. The user could specify additional configurations for the virtual service like URI rewrite rules on the primary route or any other route if desired. The user can also create specific destination rules for each of the services.","title":"Istio and Rollouts"},{"location":"features/traffic-management/istio/#integrating-with-gitops","text":"The above strategy introduces a problem for users practicing GitOps. The Rollout requires the user-defined Virtual Service to define an HTTP route with both destinations hosts. However, Istio requires routes with multiple destinations to assign a weight to each destination. Since the Argo Rollout controller modifies these Virtual Service's weights as a Rollout progresses through its steps, the Virtual Service becomes out of sync with the Git version. Additionally, if a GitOps tool does an apply after the Argo Rollouts controller changes the Virtual Service's weight, the apply would revert the weight to the percentage stored in the Git repo. At best, the user can specify the desired weight of 100% to the stable service and 0% to the canary service. In this case, the Virtual Service is synced with the Git repo when the Rollout completed all the steps. Argo CD has an open issue here to address this problem. The proposed solution is to introduce an annotation to the VirtualService which tells Argo CD controller to respect the current weights listed and let the Argo Rollouts controller manage them instead.","title":"Integrating with GitOps"},{"location":"features/traffic-management/istio/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"features/traffic-management/istio/#have-the-rollout-own-the-virtual-service","text":"Instead of the controller modifying a reference to a Virtual Service, the Rollout controller would create, manage, and own a Virtual Service. While this approach is GitOps friendly, it introduces other issues: * To provide the same flexibility as referencing Virtual Service within a Rollout, the Rollout needs to inline a large portion of the Istio spec. However, networking is outside the responsibility of the Rollout and makes the Rollout spec necessary complicated. * If Istio introduces a feature, that feature will not be available in Argo Rollouts until implemented within Argo Rollouts. Both of these issues adds more complexity to the users and Argo Rollouts developers compared to referencing a Virtual Service.","title":"Have the Rollout Own the Virtual Service"},{"location":"features/traffic-management/istio/#implement-istio-support-through-the-smi","text":"SMI is the Service Mesh Interface, which serves as a standard interface for all common features of a service mesh. This feature is GitOps friendly, but native Istio has extra functionality that SMI does not currently provide. Granted, Argo Rollouts should integrate with the SMI independent of the native Istio integration.","title":"Implement Istio support through the SMI"},{"location":"features/traffic-management/nginx/","text":"Nginx NOTE: This is being implemented for a later version and everything described below is subject to change. The Nginx Ingress Controller enables traffic management through one or more Ingress objects to configure an Nginx deployment that routes traffic directly to pods. Each Nginx Ingress contains multiple annotations that modify the behavior of the Nginx Deployment. For traffic management between different versions of an application, the Nginx Ingress controller provides the capability to split traffic by introducing a second Ingress object (referred to as the canary Ingress) with some special annotations. Here are the canary specific annotations: nginx.ingress.kubernetes.io/canary indicates that this Ingress is serving canary traffic nginx.ingress.kubernetes.io/canary-weight indicates what percentage of traffic to send to the canary. Other canary-specific annotations deal with routing traffic via headers or cookies. A future version of Argo Rollouts will contain this functionality. You can read more about these canary annotations on the official documenentation page . The canary Ingress ignores any other non-canary nginx annotations. Instead, it leverages the annotation settings from the primary Ingress. Integration with Argo Rollouts There are a couple of required fields in a Rollout to send split traffic between versions using Nginx. Below is an example of a Rollout with those fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : ... strategy : canary : canaryService : canary-service # required stableService : stable-service # required trafficRouting : nginx : primaryIngress : primary-ingress # required annotationPrefix : example.nginx.com/ # optional The primary Ingress field is a reference to an Ingress in the same namespace of the Rollout. The Rollout requires the primary Ingress routes traffic to the stable ReplicaSet. The Rollout checks that condition by confirming the Ingress has a backend that matches the Rollout's stableService. The controller routes traffic to the canary ReplicaSet by creating a second Ingress with the canary annotations. As the Rollout progresses through the Canary steps, the controller updates the canary Ingress's canary annotations to reflect the desired state of the Rollout enabling traffic splitting between two different versions. Since the Nginx Ingress controller allows users to configure the annotation prefix used by the Ingress controller, Rollouts can specify the optional annonationPrefix field. The canary Ingress uses that prefix instead of the default nginx.ingress.kubernetes.io if the field set.","title":"NGINX"},{"location":"features/traffic-management/nginx/#nginx","text":"NOTE: This is being implemented for a later version and everything described below is subject to change. The Nginx Ingress Controller enables traffic management through one or more Ingress objects to configure an Nginx deployment that routes traffic directly to pods. Each Nginx Ingress contains multiple annotations that modify the behavior of the Nginx Deployment. For traffic management between different versions of an application, the Nginx Ingress controller provides the capability to split traffic by introducing a second Ingress object (referred to as the canary Ingress) with some special annotations. Here are the canary specific annotations: nginx.ingress.kubernetes.io/canary indicates that this Ingress is serving canary traffic nginx.ingress.kubernetes.io/canary-weight indicates what percentage of traffic to send to the canary. Other canary-specific annotations deal with routing traffic via headers or cookies. A future version of Argo Rollouts will contain this functionality. You can read more about these canary annotations on the official documenentation page . The canary Ingress ignores any other non-canary nginx annotations. Instead, it leverages the annotation settings from the primary Ingress.","title":"Nginx"},{"location":"features/traffic-management/nginx/#integration-with-argo-rollouts","text":"There are a couple of required fields in a Rollout to send split traffic between versions using Nginx. Below is an example of a Rollout with those fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : ... strategy : canary : canaryService : canary-service # required stableService : stable-service # required trafficRouting : nginx : primaryIngress : primary-ingress # required annotationPrefix : example.nginx.com/ # optional The primary Ingress field is a reference to an Ingress in the same namespace of the Rollout. The Rollout requires the primary Ingress routes traffic to the stable ReplicaSet. The Rollout checks that condition by confirming the Ingress has a backend that matches the Rollout's stableService. The controller routes traffic to the canary ReplicaSet by creating a second Ingress with the canary annotations. As the Rollout progresses through the Canary steps, the controller updates the canary Ingress's canary annotations to reflect the desired state of the Rollout enabling traffic splitting between two different versions. Since the Nginx Ingress controller allows users to configure the annotation prefix used by the Ingress controller, Rollouts can specify the optional annonationPrefix field. The canary Ingress uses that prefix instead of the default nginx.ingress.kubernetes.io if the field set.","title":"Integration with Argo Rollouts"}]}